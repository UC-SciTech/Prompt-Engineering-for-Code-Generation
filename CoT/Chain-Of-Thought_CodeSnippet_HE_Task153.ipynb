{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71c150c",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: Pauline Armamento - U3246782 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Few-shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752494d",
   "metadata": {},
   "source": [
    "### HumanEval\n",
    "\n",
    "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models.\n",
    "https://huggingface.co/datasets/openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cfbd7",
   "metadata": {},
   "source": [
    "# Task ID 153\n",
    "Prompt: Write a Python function that takes the input of class name and list of extensions. Find the extension with the highest strength, calculated by the difference between uppercase and lowercase letters. Join the class name with the strongest extension using a period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3cc7e",
   "metadata": {},
   "source": [
    "### Revised Prompt for Chain Of Thought\n",
    "Prompt: Write a python code snippet that does not uses a function or function definition to find the strongest extension to load a class. Let's think step by step: \n",
    "\n",
    "1. Define Strength: The strength of an extension is the absolute difference between the number of uppercase and lowercase letters in its name. \n",
    "\n",
    "2. Iterate Extensions: Loop through a provided list of extensions.\n",
    "\n",
    "3. Calculate Strength: For each extension, calculate its strength using the rule from step 1.\n",
    "\n",
    "4. Track Strongest: Keep track of the strongest extension seen so far (initially None) and its corresponding strength.\n",
    "\n",
    "5. Compare Strength: If the current extension's strength is greater than the strongest seen yet, update the strongest extension and its strength.\n",
    "\n",
    "6. Handle Tie: If the current extension has the same strength as the strongest but appears earlier in the list (lower index), update the strongest extension (prioritizes earlier extensions).\n",
    "\n",
    "7. Finalize: After iterating through all extensions, return a string combining the class name and the strongest extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc15beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: HumanEval/153\n",
      "Prompt: \n",
      "def Strongest_Extension(class_name, extensions):\n",
      "    \"\"\"You will be given the name of a class (a string) and a list of extensions.\n",
      "    The extensions are to be used to load additional classes to the class. The\n",
      "    strength of the extension is as follows: Let CAP be the number of the uppercase\n",
      "    letters in the extension's name, and let SM be the number of lowercase letters \n",
      "    in the extension's name, the strength is given by the fraction CAP - SM. \n",
      "    You should find the strongest extension and return a string in this \n",
      "    format: ClassName.StrongestExtensionName.\n",
      "    If there are two or more extensions with the same strength, you should\n",
      "    choose the one that comes first in the list.\n",
      "    For example, if you are given \"Slices\" as the class and a list of the\n",
      "    extensions: ['SErviNGSliCes', 'Cheese', 'StuFfed'] then you should\n",
      "    return 'Slices.SErviNGSliCes' since 'SErviNGSliCes' is the strongest extension \n",
      "    (its strength is -1).\n",
      "    Example:\n",
      "    for Strongest_Extension('my_class', ['AA', 'Be', 'CC']) == 'my_class.AA'\n",
      "    \"\"\"\n",
      "\n",
      "Canonical Solution:     strong = extensions[0]\n",
      "    my_val = len([x for x in extensions[0] if x.isalpha() and x.isupper()]) - len([x for x in extensions[0] if x.isalpha() and x.islower()])\n",
      "    for s in extensions:\n",
      "        val = len([x for x in s if x.isalpha() and x.isupper()]) - len([x for x in s if x.isalpha() and x.islower()])\n",
      "        if val > my_val:\n",
      "            strong = s\n",
      "            my_val = val\n",
      "\n",
      "    ans = class_name + \".\" + strong\n",
      "    return ans\n",
      "\n",
      "\n",
      "Test Data: def check(candidate):\n",
      "\n",
      "    # Check some simple cases\n",
      "    assert candidate('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']) == 'Watashi.eIGHt8OKe'\n",
      "    assert candidate('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) == 'Boku123.YEs.WeCaNe'\n",
      "    assert candidate('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) == '__YESIMHERE.NuLl__'\n",
      "    assert candidate('K', ['Ta', 'TAR', 't234An', 'cosSo']) == 'K.TAR'\n",
      "    assert candidate('__HAHA', ['Tab', '123', '781345', '-_-']) == '__HAHA.123'\n",
      "    assert candidate('YameRore', ['HhAas', 'okIWILL123', 'WorkOut', 'Fails', '-_-']) == 'YameRore.okIWILL123'\n",
      "    assert candidate('finNNalLLly', ['Die', 'NowW', 'Wow', 'WoW']) == 'finNNalLLly.WoW'\n",
      "\n",
      "    # Check some edge cases that are easy to work out by hand.\n",
      "    assert candidate('_', ['Bb', '91245']) == '_.Bb'\n",
      "    assert candidate('Sp', ['671235', 'Bb']) == 'Sp.671235'\n",
      "    \n",
      "\n",
      "Entry Point: Strongest_Extension\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "# Initialize Task Index\n",
    "task_index = 153\n",
    "task_153 = dataset[\"test\"][task_index]\n",
    "\n",
    "# Access Task Details\n",
    "task_id = task_153[\"task_id\"]\n",
    "prompt = task_153[\"prompt\"]\n",
    "canonical_solution = task_153[\"canonical_solution\"]\n",
    "test_data = task_153[\"test\"]\n",
    "entry_point = task_153[\"entry_point\"]\n",
    "\n",
    "# Display Task Details\n",
    "print(f\"Task ID: {task_id}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Canonical Solution: {canonical_solution}\")\n",
    "print(f\"Test Data: {test_data}\")\n",
    "print(f\"Entry Point: {entry_point}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383eda64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs:\n",
      "mod1_output1: Watashi.eIGHt8OKe\n",
      "mod1_output2: Boku123.nani\n",
      "mod1_output3: __YESIMHERE.nothing\n",
      "\n",
      "Stored execution times:\n",
      "execution_time_mod1_output1: 0.00000\n",
      "execution_time_mod1_output2: 0.00000\n",
      "execution_time_mod1_output3: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - CoT\n",
    "# Model 1 \n",
    "\n",
    "import time\n",
    "\n",
    "# Define input pairs\n",
    "inputs = [\n",
    "    ('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']),\n",
    "    ('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']),\n",
    "    ('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321'])\n",
    "]\n",
    "\n",
    "# Initialize variables to store outputs\n",
    "mod1_output1 = None\n",
    "mod1_output2 = None\n",
    "mod1_output3 = None\n",
    "\n",
    "# Initialize variables to store execution times\n",
    "execution_time_mod1_output1 = None\n",
    "execution_time_mod1_output2 = None\n",
    "execution_time_mod1_output3 = None\n",
    "\n",
    "# Loop over inputs and measure execution time\n",
    "for index, input_data in enumerate(inputs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    class_name, extensions = input_data\n",
    "\n",
    "    strongest_extension = None\n",
    "    strongest_strength = 0\n",
    "\n",
    "    for ext in extensions:\n",
    "        uppercase_count = sum(1 for char in ext if char.isupper())\n",
    "        lowercase_count = sum(1 for char in ext if char.islower())\n",
    "        strength = abs(uppercase_count - lowercase_count)\n",
    "        \n",
    "        if strength > strongest_strength or (strength == strongest_strength and extensions.index(ext) < extensions.index(strongest_extension)):\n",
    "            strongest_extension = ext\n",
    "            strongest_strength = strength\n",
    "\n",
    "    result = class_name + \".\" + strongest_extension\n",
    "    \n",
    "    # Store the output and execution time based on the index\n",
    "    if index == 0:\n",
    "        mod1_output1 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod1_output1 = time.time() - start_time\n",
    "    elif index == 1:\n",
    "        mod1_output2 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod1_output2 = time.time() - start_time\n",
    "    elif index == 2:\n",
    "        mod1_output3 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod1_output3 = time.time() - start_time\n",
    "\n",
    "# Print the stored outputs\n",
    "print(\"Stored outputs:\")\n",
    "print(\"mod1_output1:\", mod1_output1)\n",
    "print(\"mod1_output2:\", mod1_output2)\n",
    "print(\"mod1_output3:\", mod1_output3)\n",
    "\n",
    "print()\n",
    "# Print the stored execution times\n",
    "print(\"Stored execution times:\")\n",
    "print(\"execution_time_mod1_output1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "print(\"execution_time_mod1_output2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "print(\"execution_time_mod1_output3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3b1bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.00000\n",
      "Execution time Model 1 Output 2: 0.00000\n",
      "Execution time Model 1 Output 3: 0.00000\n",
      "Average Execution Time for Model 1 Output: 0.00000\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.nani', '__YESIMHERE.nothing']\n",
      "                 Precision    Recall  F1 Score  Accuracy  Exact Match\n",
      "0                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "1                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "2                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "Model 1 Average   0.333333  0.333333  0.333333  0.333333     0.333333\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/20\n",
    "# Function - Zero Shot\n",
    "# Overall Metrics\n",
    "\n",
    "## Model 1 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    if actual_output is None:\n",
    "        if expected_output is None:\n",
    "            return {\n",
    "                \"Precision\": 1,\n",
    "                \"Recall\": 1,\n",
    "                \"F1 Score\": 1,\n",
    "                \"Accuracy\": 1,\n",
    "                \"Exact Match\": 1\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"Precision\": 0,\n",
    "                \"Recall\": 0,\n",
    "                \"F1 Score\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Exact Match\": 0\n",
    "            }\n",
    "    \n",
    "    if isinstance(actual_output, list):\n",
    "        if None in actual_output:\n",
    "            actual_output = [x for x in actual_output if x is not None]\n",
    "\n",
    "        true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "        false_positives = max(len(actual_output) - true_positives, 0)\n",
    "        false_negatives = max(len(expected_output) - true_positives, 0)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "    else:\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "        return {\n",
    "            \"Precision\": exact_match_accuracy,\n",
    "            \"Recall\": exact_match_accuracy,\n",
    "            \"F1 Score\": exact_match_accuracy,\n",
    "            \"Accuracy\": exact_match_accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7cb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs:\n",
      "mod2_output1: Watashi.eIGHt8OKe\n",
      "mod2_output2: Boku123.nani\n",
      "mod2_output3: __YESIMHERE.nothing\n",
      "\n",
      "Stored execution times:\n",
      "execution_time_mod2_output1: 0.00000\n",
      "execution_time_mod2_output2: 0.00000\n",
      "execution_time_mod2_output3: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - CoT\n",
    "# Model 2 \n",
    "\n",
    "import time\n",
    "\n",
    "# Define input pairs\n",
    "inputs = [\n",
    "    ('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']),\n",
    "    ('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']),\n",
    "    ('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321'])\n",
    "]\n",
    "\n",
    "# Initialize variables to store outputs and execution times\n",
    "mod2_output1 = None\n",
    "mod2_output2 = None\n",
    "mod2_output3 = None\n",
    "execution_time_mod2_output1 = None\n",
    "execution_time_mod2_output2 = None\n",
    "execution_time_mod2_output3 = None\n",
    "\n",
    "# Loop over inputs and measure execution time\n",
    "for index, input_data in enumerate(inputs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    class_name, extensions = input_data\n",
    "\n",
    "    strongest_extension = None\n",
    "    strongest_strength = 0\n",
    "    strongest_index = 0\n",
    "\n",
    "    # Iterate through extensions and calculate strength\n",
    "    for i, ext in enumerate(extensions):\n",
    "        uppercase_count = sum(1 for char in ext if char.isupper())\n",
    "        lowercase_count = sum(1 for char in ext if char.islower())\n",
    "        strength = abs(uppercase_count - lowercase_count)\n",
    "        \n",
    "        # Update strongest extension\n",
    "        if strength > strongest_strength or (strength == strongest_strength and i < strongest_index):\n",
    "            strongest_extension = ext\n",
    "            strongest_strength = strength\n",
    "            strongest_index = i\n",
    "\n",
    "    # Combine class name and strongest extension\n",
    "    result = class_name + \".\" + strongest_extension\n",
    "\n",
    "    # Store the output and execution time based on the index\n",
    "    if index == 0:\n",
    "        mod2_output1 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod2_output1 = time.time() - start_time\n",
    "    elif index == 1:\n",
    "        mod2_output2 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod2_output2 = time.time() - start_time\n",
    "    elif index == 2:\n",
    "        mod2_output3 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod2_output3 = time.time() - start_time\n",
    "\n",
    "# Print the stored outputs\n",
    "print(\"Stored outputs:\")\n",
    "print(\"mod2_output1:\", mod2_output1)\n",
    "print(\"mod2_output2:\", mod2_output2)\n",
    "print(\"mod2_output3:\", mod2_output3)\n",
    "\n",
    "print()\n",
    "# Print the stored execution times\n",
    "print(\"Stored execution times:\")\n",
    "print(\"execution_time_mod2_output1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "print(\"execution_time_mod2_output2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "print(\"execution_time_mod2_output3:\", format(execution_time_mod2_output3, '.5f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20cb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.00000\n",
      "Execution time Model 2 Output 2: 0.00000\n",
      "Execution time Model 2 Output 3: 0.00000\n",
      "Average Execution Time for Model 2 Output: 0.00000\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.nani', '__YESIMHERE.nothing']\n",
      "                 Precision    Recall  F1 Score  Accuracy  Exact Match\n",
      "0                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "1                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "2                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "Model 2 Average   0.333333  0.333333  0.333333  0.333333     0.333333\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function - Zero Shot\n",
    "\n",
    "## Model 2 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(actual_output, list):\n",
    "        true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "        false_positives = max(len(actual_output) - true_positives, 0)\n",
    "        false_negatives = max(len(expected_output) - true_positives, 0)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "    else:\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "        return {\n",
    "            \"Precision\": exact_match_accuracy,\n",
    "            \"Recall\": exact_match_accuracy,\n",
    "            \"F1 Score\": exact_match_accuracy,\n",
    "            \"Accuracy\": exact_match_accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod2 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod2)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod2)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics2_new = pd.concat([df_metrics2_new, avg_metrics2.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eec047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs:\n",
      "mod3_output1: Watashi.eIGHt8OKe\n",
      "mod3_output2: Boku123.nani\n",
      "mod3_output3: __YESIMHERE.nothing\n",
      "\n",
      "Stored execution times:\n",
      "execution_time_mod3_output1: 0.00000\n",
      "execution_time_mod3_output2: 0.00000\n",
      "execution_time_mod3_output3: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 3\n",
    "\n",
    "import time\n",
    "\n",
    "# Define input pairs\n",
    "inputs = [\n",
    "    ('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']),\n",
    "    ('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']),\n",
    "    ('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321'])\n",
    "]\n",
    "\n",
    "# Initialize variables to store outputs and execution times\n",
    "mod3_output1 = None\n",
    "mod3_output2 = None\n",
    "mod3_output3 = None\n",
    "execution_time_mod3_output1 = None\n",
    "execution_time_mod3_output2 = None\n",
    "execution_time_mod3_output3 = None\n",
    "\n",
    "# Loop over inputs and measure execution time\n",
    "index = 0\n",
    "for class_name, extensions in inputs:\n",
    "    start_time = time.time()\n",
    "\n",
    "    strongest_extension = None\n",
    "    strongest_strength = 0\n",
    "    strongest_index = 0\n",
    "\n",
    "    # Iterate through extensions and calculate strength\n",
    "    for i in range(len(extensions)):\n",
    "        ext = extensions[i]\n",
    "        uppercase_count = sum(1 for char in ext if char.isupper())\n",
    "        lowercase_count = sum(1 for char in ext if char.islower())\n",
    "        strength = abs(uppercase_count - lowercase_count)\n",
    "        \n",
    "        # Update strongest extension\n",
    "        if strength > strongest_strength or (strength == strongest_strength and i < strongest_index):\n",
    "            strongest_extension = ext\n",
    "            strongest_strength = strength\n",
    "            strongest_index = i\n",
    "\n",
    "    # Combine class name and strongest extension\n",
    "    result = class_name + \".\" + strongest_extension\n",
    "\n",
    "    # Store the output and execution time based on the index\n",
    "    if index == 0:\n",
    "        mod3_output1 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod3_output1 = time.time() - start_time\n",
    "    elif index == 1:\n",
    "        mod3_output2 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod3_output2 = time.time() - start_time\n",
    "    elif index == 2:\n",
    "        mod3_output3 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod3_output3 = time.time() - start_time\n",
    "\n",
    "    # Move to the next input pair\n",
    "    index += 1\n",
    "\n",
    "# Print the stored outputs\n",
    "print(\"Stored outputs:\")\n",
    "print(\"mod3_output1:\", mod3_output1)\n",
    "print(\"mod3_output2:\", mod3_output2)\n",
    "print(\"mod3_output3:\", mod3_output3)\n",
    "\n",
    "print()\n",
    "# Print the stored execution times\n",
    "print(\"Stored execution times:\")\n",
    "print(\"execution_time_mod3_output1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "print(\"execution_time_mod3_output2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "print(\"execution_time_mod3_output3:\", format(execution_time_mod3_output3, '.5f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec8f45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.00000\n",
      "Execution time Model 3 Output 2: 0.00000\n",
      "Execution time Model 3 Output 3: 0.00000\n",
      "Average Execution Time for Model 3 Output: 0.00000\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.nani', '__YESIMHERE.nothing']\n",
      "                 Precision    Recall  F1 Score  Accuracy  Exact Match\n",
      "0                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "1                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "2                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "Model 3 Average   0.333333  0.333333  0.333333  0.333333     0.333333\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function - Zero Shot\n",
    "# Model 3 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    if actual_output is None:\n",
    "        if expected_output is None:\n",
    "            return {\n",
    "                \"Precision\": 1,\n",
    "                \"Recall\": 1,\n",
    "                \"F1 Score\": 1,\n",
    "                \"Accuracy\": 1,\n",
    "                \"Exact Match\": 1\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"Precision\": 0,\n",
    "                \"Recall\": 0,\n",
    "                \"F1 Score\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Exact Match\": 0\n",
    "            }\n",
    "    \n",
    "    if isinstance(actual_output, list):\n",
    "        if None in actual_output:\n",
    "            actual_output = [x for x in actual_output if x is not None]\n",
    "\n",
    "        true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "        false_positives = max(len(actual_output) - true_positives, 0)\n",
    "        false_negatives = max(len(expected_output) - true_positives, 0)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "    else:\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "        return {\n",
    "            \"Precision\": exact_match_accuracy,\n",
    "            \"Recall\": exact_match_accuracy,\n",
    "            \"F1 Score\": exact_match_accuracy,\n",
    "            \"Accuracy\": exact_match_accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod3 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod3)\n",
    "\n",
    "metrics3 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod3)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics3_new = pd.DataFrame(metrics3)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "avg_metrics3.name = 'Model 3 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics3_new = pd.concat([df_metrics3_new, avg_metrics3.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics3_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec7a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Precision    Recall  F1 Score  Accuracy  Exact Match  \\\n",
      "Model 1                0.333333  0.333333  0.333333  0.333333     0.333333   \n",
      "Model 2                0.333333  0.333333  0.333333  0.333333     0.333333   \n",
      "Model 3                0.333333  0.333333  0.333333  0.333333     0.333333   \n",
      "Overall Average        0.333333  0.333333  0.333333  0.333333     0.333333   \n",
      "Overall Average Time   0.000000  0.000000  0.000000  0.000000     0.000000   \n",
      "\n",
      "                      Average Time  \n",
      "Model 1                        0.0  \n",
      "Model 2                        0.0  \n",
      "Model 3                        0.0  \n",
      "Overall Average                0.0  \n",
      "Overall Average Time           0.0  \n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Overall Metrics\n",
    "\n",
    "# Calculate the averages for each set of metrics\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "\n",
    "# Calculate the overall average time\n",
    "overall_average_time = (average_time_mod1output + average_time_mod2output + average_time_mod3output) / 3\n",
    "\n",
    "# Create a DataFrame to store the averages\n",
    "df_avg_metrics = pd.DataFrame([avg_metrics1, avg_metrics2, avg_metrics3])\n",
    "\n",
    "# Add a new column for average time\n",
    "df_avg_metrics['Average Time'] = [average_time_mod1output, average_time_mod2output, average_time_mod3output]\n",
    "\n",
    "# Add a new row displaying overall average\n",
    "df_avg_metrics.loc['Overall Average'] = df_avg_metrics.mean()\n",
    "\n",
    "# Add a new row for overall average time\n",
    "df_avg_metrics.loc['Overall Average Time'] = overall_average_time\n",
    "\n",
    "# Rename the index to include model names\n",
    "df_avg_metrics.index = ['Model 1', 'Model 2', 'Model 3', 'Overall Average', 'Overall Average Time']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ec726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bfc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
