{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ae1e6a",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: MD Alvee Rohan - U3235512 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Chain Of Thought </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf000b1f",
   "metadata": {},
   "source": [
    "### Revised Prompt for COT\n",
    "\n",
    "### BIG-bench\\\\ c114\n",
    "\n",
    "We want to create a function that takes a list of natural numbers and creates a new list where the last element of the original list is prepended (added to the beginning). Let's think step by step: \n",
    "\n",
    "1. Access Last Element: How can we access the last element of the list? We can use indexing or built-in methods.\n",
    "\n",
    "2. Create New List: We need to create a new list to store the modified version.\n",
    "\n",
    "3. Prepend Last Element:  We need to add the last element we obtained in step 1 to the beginning of the new list. We can use indexing or methods like list.insert(0, element) (which inserts an element at a specific index).\n",
    "\n",
    "4. Append Remaining Elements:  The new list should also include the original elements except for the last one. We can iterate through the original list (excluding the last element) and append each element to the new list.\n",
    "\n",
    "5. Return New List: After completing steps 1-4, the new list with the prepended last element is ready. We can return this new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184da9cd-0b92-4f47-8cdf-f7712f72163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.01457\n",
      "Execution time Model 1 Output 2: 0.01286\n",
      "Execution time Model 1 Output 3: 0.01304\n",
      "Average Execution Time for Model 1 Output: 0.01349\n",
      "Expected Output:  [[28, 8, 28], [85, 0, 67, 85], [47, 0, 32, 9, 47]]\n",
      "Actual Output:  [[28, 8], [85, 0, 67], [47, 0, 32, 9]]\n",
      "   Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0        1.0     1.0       1.0  0.666667            0\n",
      "1        1.0     1.0       1.0  0.750000            0\n",
      "2        1.0     1.0       1.0  0.800000            0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c114\n",
    "# Function Generation - COT\n",
    "# Model 1\n",
    "\n",
    "import timeit\n",
    "\n",
    "def prepend_last_element_model1(input_list):\n",
    "    # Step 1: Access Last Element\n",
    "    last_element = input_list[-1]\n",
    "\n",
    "    # Step 2: Create New List\n",
    "    new_list = []\n",
    "\n",
    "    # Step 3: Prepend Last Element\n",
    "    new_list.append(last_element)\n",
    "\n",
    "    # Step 4: Append Remaining Elements\n",
    "    for element in input_list[:-1]:\n",
    "        new_list.append(element)\n",
    "\n",
    "    # Step 5: Return New List\n",
    "    return new_list\n",
    "\n",
    "mod1_input1 = [8, 28] #  [28, 8, 28]\n",
    "mod1_input2 = [0, 67, 85] # [85, 0, 67, 85]\n",
    "mod1_input3 = [0, 32, 9, 47] # [47, 0, 32, 9, 47]\n",
    "\n",
    "mod1_output1 = prepend_last_element_model1(mod1_input1)\n",
    "mod1_output2 = prepend_last_element_model1(mod1_input2)\n",
    "mod1_output3 = prepend_last_element_model1(mod1_input3)\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod1_output1 = timeit.timeit(lambda: prepend_last_element_model1(mod1_input1), number=10000)\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "execution_time_mod1_output2 = timeit.timeit(lambda: prepend_last_element_model1(mod1_input2), number=10000)\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "execution_time_mod1_output3 = timeit.timeit(lambda: prepend_last_element_model1(mod1_input3), number=10000)\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "import pandas as pd\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    true_positives = sum(1 for num in actual_output if num in expected_output)\n",
    "    false_positives = sum(1 for num in actual_output if num not in expected_output)\n",
    "    false_negatives = sum(1 for num in expected_output if num not in actual_output)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for num in actual_output if num in expected_output) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "expected_outputs_c114 = [[28, 8, 28], [85, 0, 67, 85], [47, 0, 32, 9, 47]]\n",
    "actual_outputs_mod1c114 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs_c114)\n",
    "print(\"Actual Output: \", actual_outputs_mod1c114)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs_c114, actual_outputs_mod1c114)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e469a154-afeb-41df-ad8b-b71352e30a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.01754\n",
      "Execution time Model 2 Output 2: 0.01978\n",
      "Execution time Model 2 Output 3: 0.02311\n",
      "Average Execution Time for Model 2 Output: 0.02014\n",
      "Expected Output:  [[28, 8, 28], [85, 0, 67, 85], [47, 0, 32, 9, 47]]\n",
      "Actual Output:  [[28, 8], [85, 0, 67], [47, 0, 32, 9]]\n",
      "   Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0        1.0     1.0       1.0  0.666667            0\n",
      "1        1.0     1.0       1.0  0.750000            0\n",
      "2        1.0     1.0       1.0  0.800000            0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c114\n",
    "# Function Generation - COT\n",
    "# Model 2\n",
    "\n",
    "import timeit\n",
    "\n",
    "def prepend_last_element_model2(input_list):\n",
    "    # Access Last Element and Create New List\n",
    "    last_element = input_list[-1]\n",
    "    new_list = [last_element]\n",
    "\n",
    "    # Append Remaining Elements and Return New List\n",
    "    new_list.extend(input_list[:-1])\n",
    "    return new_list\n",
    "mod2_input1 = [8, 28] #  [28, 8, 28]\n",
    "mod2_input2 = [0, 67, 85] # [85, 0, 67, 85]\n",
    "mod2_input3 = [0, 32, 9, 47] # [47, 0, 32, 9, 47]\n",
    "\n",
    "mod2_output1 = prepend_last_element_model2(mod2_input1)\n",
    "mod2_output2 = prepend_last_element_model2(mod2_input2)\n",
    "mod2_output3 = prepend_last_element_model2(mod2_input3)\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod2_output1 = timeit.timeit(lambda: prepend_last_element_model2(mod2_input1), number=10000)\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "execution_time_mod2_output2 = timeit.timeit(lambda: prepend_last_element_model2(mod2_input2), number=10000)\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "execution_time_mod2_output3 = timeit.timeit(lambda: prepend_last_element_model2(mod2_input3), number=10000)\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "import pandas as pd\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    \n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "    false_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 1 and actual_output[i] == 0)\n",
    "    false_negatives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 0 and actual_output[i] == 1)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "expected_outputs_c114 = [[28, 8, 28], [85, 0, 67, 85], [47, 0, 32, 9, 47]]\n",
    "actual_outputs_mod2c114 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs_c114)\n",
    "print(\"Actual Output: \", actual_outputs_mod2c114)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs_c114, actual_outputs_mod2c114)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa56d05-a755-4187-9d9f-f3079813e8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.01323\n",
      "Execution time Model 3 Output 2: 0.00753\n",
      "Execution time Model 3 Output 3: 0.01271\n",
      "Average Execution Time for Model 3 Output: 0.01115\n",
      "Expected Output:  [[28, 8, 28], [85, 0, 67, 85], [47, 0, 32, 9, 47]]\n",
      "Actual Output:  [[28, 8], [85, 0, 67], [47, 0, 32, 9]]\n",
      "   Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0        1.0     1.0       1.0  0.666667            0\n",
      "1        1.0     1.0       1.0  0.750000            0\n",
      "2        1.0     1.0       1.0  0.800000            0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c114\n",
    "# Function Generation - COT\n",
    "# Model 3\n",
    "\n",
    "import timeit\n",
    "def prepend_last_element_model3(input_list):\n",
    "    # Step 1: Access Last Element\n",
    "    last_element = input_list[-1]\n",
    "\n",
    "    # Step 2: Create New List\n",
    "    new_list = [last_element]\n",
    "\n",
    "    # Step 3: Append Remaining Elements\n",
    "    new_list += input_list[:-1]\n",
    "\n",
    "    # Step 4: Return New List\n",
    "    return new_list\n",
    "\n",
    "mod3_input1 = [8, 28] #  [28, 8, 28]\n",
    "mod3_input2 = [0, 67, 85] # [85, 0, 67, 85]\n",
    "mod3_input3 = [0, 32, 9, 47] # [47, 0, 32, 9, 47]\n",
    "\n",
    "mod3_output1 = prepend_last_element_model3(mod3_input1)\n",
    "mod3_output2 = prepend_last_element_model3(mod3_input2)\n",
    "mod3_output3 = prepend_last_element_model3(mod3_input3)\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod3_output1 = timeit.timeit(lambda: prepend_last_element_model3(mod3_input1), number=10000)\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "execution_time_mod3_output2 = timeit.timeit(lambda: prepend_last_element_model3(mod3_input2), number=10000)\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "execution_time_mod3_output3 = timeit.timeit(lambda: prepend_last_element_model3(mod3_input3), number=10000)\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output3, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "import pandas as pd\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    \n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "    false_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 1 and actual_output[i] == 0)\n",
    "    false_negatives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 0 and actual_output[i] == 1)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs_c114 = [[28, 8, 28], [85, 0, 67, 85], [47, 0, 32, 9, 47]]\n",
    "actual_outputs_mod3c114 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs_c114)\n",
    "print(\"Actual Output: \", actual_outputs_mod3c114)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs_c114, actual_outputs_mod3c114)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5460577-7051-4c12-a5f9-f5ac9d397471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
