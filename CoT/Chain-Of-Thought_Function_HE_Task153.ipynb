{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71c150c",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: Pauline Armamento - U3246782 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Few-shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752494d",
   "metadata": {},
   "source": [
    "### HumanEval\n",
    "\n",
    "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models.\n",
    "https://huggingface.co/datasets/openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3366345",
   "metadata": {},
   "source": [
    "# Task ID 153\n",
    "Prompt: Write a Python function that takes the input of class name and list of extensions. Find the extension with the highest strength, calculated by the difference between uppercase and lowercase letters. Join the class name with the strongest extension using a period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d95efc",
   "metadata": {},
   "source": [
    "### Revised Prompt for Chain Of Thought\n",
    "Prompt: We want to find the strongest extension for a class, considering the number of uppercase letters in the extension name. Let's think step by step: \n",
    "\n",
    "1. Think about Strength: Define the concept of strength for an extension. How can we represent the strength of an extension based on its name? Here, we can use the difference between uppercase and lowercase letters\n",
    "\n",
    "2. Calculate Strength: For each extension, calculate its strength. How many uppercase letters does it have? How many lowercase letters? Can you write a formula to represent this strength? (Strength = Uppercase count - Lowercase count)\n",
    "\n",
    "3. Find the Strongest: Now that we know the strength of each extension, how can we find the strongest one in the list? We should select the one with the highest strength. What if there's a tie? We choose the one with the first highest strength extension in the list.\n",
    "\n",
    "4. Combine and Return: Finally, we want to return the class name along with the strongest extension. How can we format this as a string? (ClassName + “.” + StrongestExtension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc15beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: HumanEval/153\n",
      "Prompt: \n",
      "def Strongest_Extension(class_name, extensions):\n",
      "    \"\"\"You will be given the name of a class (a string) and a list of extensions.\n",
      "    The extensions are to be used to load additional classes to the class. The\n",
      "    strength of the extension is as follows: Let CAP be the number of the uppercase\n",
      "    letters in the extension's name, and let SM be the number of lowercase letters \n",
      "    in the extension's name, the strength is given by the fraction CAP - SM. \n",
      "    You should find the strongest extension and return a string in this \n",
      "    format: ClassName.StrongestExtensionName.\n",
      "    If there are two or more extensions with the same strength, you should\n",
      "    choose the one that comes first in the list.\n",
      "    For example, if you are given \"Slices\" as the class and a list of the\n",
      "    extensions: ['SErviNGSliCes', 'Cheese', 'StuFfed'] then you should\n",
      "    return 'Slices.SErviNGSliCes' since 'SErviNGSliCes' is the strongest extension \n",
      "    (its strength is -1).\n",
      "    Example:\n",
      "    for Strongest_Extension('my_class', ['AA', 'Be', 'CC']) == 'my_class.AA'\n",
      "    \"\"\"\n",
      "\n",
      "Canonical Solution:     strong = extensions[0]\n",
      "    my_val = len([x for x in extensions[0] if x.isalpha() and x.isupper()]) - len([x for x in extensions[0] if x.isalpha() and x.islower()])\n",
      "    for s in extensions:\n",
      "        val = len([x for x in s if x.isalpha() and x.isupper()]) - len([x for x in s if x.isalpha() and x.islower()])\n",
      "        if val > my_val:\n",
      "            strong = s\n",
      "            my_val = val\n",
      "\n",
      "    ans = class_name + \".\" + strong\n",
      "    return ans\n",
      "\n",
      "\n",
      "Test Data: def check(candidate):\n",
      "\n",
      "    # Check some simple cases\n",
      "    assert candidate('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']) == 'Watashi.eIGHt8OKe'\n",
      "    assert candidate('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) == 'Boku123.YEs.WeCaNe'\n",
      "    assert candidate('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) == '__YESIMHERE.NuLl__'\n",
      "    assert candidate('K', ['Ta', 'TAR', 't234An', 'cosSo']) == 'K.TAR'\n",
      "    assert candidate('__HAHA', ['Tab', '123', '781345', '-_-']) == '__HAHA.123'\n",
      "    assert candidate('YameRore', ['HhAas', 'okIWILL123', 'WorkOut', 'Fails', '-_-']) == 'YameRore.okIWILL123'\n",
      "    assert candidate('finNNalLLly', ['Die', 'NowW', 'Wow', 'WoW']) == 'finNNalLLly.WoW'\n",
      "\n",
      "    # Check some edge cases that are easy to work out by hand.\n",
      "    assert candidate('_', ['Bb', '91245']) == '_.Bb'\n",
      "    assert candidate('Sp', ['671235', 'Bb']) == 'Sp.671235'\n",
      "    \n",
      "\n",
      "Entry Point: Strongest_Extension\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "# Initialize Task Index\n",
    "task_index = 153\n",
    "task_153 = dataset[\"test\"][task_index]\n",
    "\n",
    "# Access Task Details\n",
    "task_id = task_153[\"task_id\"]\n",
    "prompt = task_153[\"prompt\"]\n",
    "canonical_solution = task_153[\"canonical_solution\"]\n",
    "test_data = task_153[\"test\"]\n",
    "entry_point = task_153[\"entry_point\"]\n",
    "\n",
    "# Display Task Details\n",
    "print(f\"Task ID: {task_id}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Canonical Solution: {canonical_solution}\")\n",
    "print(f\"Test Data: {test_data}\")\n",
    "print(f\"Entry Point: {entry_point}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7cb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.04431\n",
      "Execution time Model 1 Output 2: 0.06819\n",
      "Execution time Model 1 Output 3: 0.08926\n",
      "Average Execution Time for Model 1 Output: 0.06725\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 1 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function - CoT\n",
    "# Model 1\n",
    "\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "def Strongest_Extension1(class_name, extensions):\n",
    "    strongest_extension = None\n",
    "    max_strength = float('-inf')\n",
    "\n",
    "    for extension in extensions:\n",
    "        uppercase_count = sum(1 for char in extension if char.isupper())\n",
    "        lowercase_count = sum(1 for char in extension if char.islower())\n",
    "        strength = uppercase_count - lowercase_count\n",
    "\n",
    "        if strength > max_strength:\n",
    "            max_strength = strength\n",
    "            strongest_extension = extension\n",
    "        elif strength == max_strength:\n",
    "            continue  # If tie, keep the first highest strength extension\n",
    "\n",
    "    return f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "\n",
    "# Test cases\n",
    "mod1_output1 = Strongest_Extension1('Watashi', ['tEN', 'niNE', 'eIGHt8OKe'])  # Output: 'Watashi.eIGHt8OKe'\n",
    "mod1_output2 = Strongest_Extension1('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) # Output: 'Boku123.YEs.WeCaNe'\n",
    "mod1_output3 = Strongest_Extension1('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) # Output: '__YESIMHERE.NuLl__'\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod1_output1 = timeit.timeit(lambda: Strongest_Extension1('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']), number=10000)\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "execution_time_mod1_output2 = timeit.timeit(lambda: Strongest_Extension1('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']), number=10000)\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "\n",
    "execution_time_mod1_output3 = timeit.timeit(lambda: Strongest_Extension1('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']), number=10000)\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20cb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.03632\n",
      "Execution time Model 2 Output 2: 0.03561\n",
      "Execution time Model 2 Output 3: 0.04420\n",
      "Average Execution Time for Model 2 Output: 0.03871\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.tEN', 'Boku123.YEs.WeCaNe', '__YESIMHERE.eMptY']\n",
      "                 Precision    Recall  F1 Score  Accuracy  Exact Match\n",
      "0                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "1                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "2                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "Model 2 Average   0.333333  0.333333  0.333333  0.333333     0.333333\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function - CoT\n",
    "# Model 2\n",
    "\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_strength(extension):\n",
    "    uppercase_count = sum(1 for char in extension if char.isupper())\n",
    "    total_letters_count = len(extension)\n",
    "    if total_letters_count == 0:  # Handle division by zero\n",
    "        return 0\n",
    "    return uppercase_count / total_letters_count\n",
    "\n",
    "\n",
    "def Strongest_Extension2(class_name, extensions):\n",
    "    strongest_extension = None\n",
    "    max_strength = float('-inf')\n",
    "\n",
    "    for extension in extensions:\n",
    "        strength = calculate_strength(extension)\n",
    "        \n",
    "        if strength > max_strength:\n",
    "            max_strength = strength\n",
    "            strongest_extension = extension\n",
    "        elif strength == max_strength:\n",
    "            continue  # If tie, keep the first highest strength extension\n",
    "\n",
    "    return f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "\n",
    "\n",
    "# Test cases\n",
    "mod2_output1 = Strongest_Extension2('Watashi', ['tEN', 'niNE', 'eIGHt8OKe'])  # Output: 'Watashi.eIGHt8OKe'\n",
    "mod2_output2 = Strongest_Extension2('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) # Output: 'Boku123.YEs.WeCaNe'\n",
    "mod2_output3 = Strongest_Extension2('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) # Output: '__YESIMHERE.NuLl__'\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod2_output1 = timeit.timeit(lambda: Strongest_Extension2('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']), number=10000)\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "execution_time_mod2_output2 = timeit.timeit(lambda: Strongest_Extension2('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']), number=10000)\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "execution_time_mod2_output3 = timeit.timeit(lambda: Strongest_Extension2('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']), number=10000)\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, str):\n",
    "        expected_output = [expected_output]\n",
    "    if isinstance(actual_output, str):\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod2 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod2)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod2)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics2_new = pd.concat([df_metrics2_new, avg_metrics2.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8f45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.05189\n",
      "Execution time Model 3 Output 2: 0.07278\n",
      "Execution time Model 3 Output 3: 0.09511\n",
      "Average Execution Time for Model 3 Output: 0.07326\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 3 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function - CoT\n",
    "# Model 3\n",
    "\n",
    "def calculate_strength2(extension):\n",
    "    uppercase_sum = sum(ord(char) for char in extension if char.isupper())\n",
    "    lowercase_sum = sum(ord(char) for char in extension if char.islower())\n",
    "    return uppercase_sum - lowercase_sum\n",
    "\n",
    "\n",
    "def Strongest_Extension3(class_name, extensions):\n",
    "    strongest_extension = None\n",
    "    max_strength = float('-inf')\n",
    "\n",
    "    for extension in extensions:\n",
    "        strength = calculate_strength2(extension)\n",
    "        \n",
    "        if strength > max_strength:\n",
    "            max_strength = strength\n",
    "            strongest_extension = extension\n",
    "        elif strength == max_strength:\n",
    "            continue  # If tie, keep the first highest strength extension\n",
    "\n",
    "    return f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "\n",
    "\n",
    "# Test cases\n",
    "mod3_output1 = Strongest_Extension3('Watashi', ['tEN', 'niNE', 'eIGHt8OKe'])  # Output: 'Watashi.eIGHt8OKe'\n",
    "mod3_output2 = Strongest_Extension3('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) # Output: 'Boku123.YEs.WeCaNe'\n",
    "mod3_output3 = Strongest_Extension3('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) # Output: '__YESIMHERE.NuLl__'\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod3_output1 = timeit.timeit(lambda: Strongest_Extension3('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']), number=10000)\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "execution_time_mod3_output2 = timeit.timeit(lambda: Strongest_Extension3('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']), number=10000)\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "execution_time_mod3_output3 = timeit.timeit(lambda: Strongest_Extension3('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']), number=10000)\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output3, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod3 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod3)\n",
    "\n",
    "metrics3 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod3)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics3_new = pd.DataFrame(metrics3)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "avg_metrics3.name = 'Model 3 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics3_new = pd.concat([df_metrics3_new, avg_metrics3.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics3_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec7a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Precision    Recall  F1 Score  Accuracy  Exact Match  \\\n",
      "Model 1                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Model 2                0.333333  0.333333  0.333333  0.333333     0.333333   \n",
      "Model 3                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Overall Average        0.777778  0.777778  0.777778  0.777778     0.777778   \n",
      "Overall Average Time   0.059741  0.059741  0.059741  0.059741     0.059741   \n",
      "\n",
      "                      Average Time  \n",
      "Model 1                   0.067255  \n",
      "Model 2                   0.038709  \n",
      "Model 3                   0.073260  \n",
      "Overall Average           0.059741  \n",
      "Overall Average Time      0.059741  \n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Overall Metrics\n",
    "\n",
    "# Calculate the averages for each set of metrics\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "\n",
    "# Calculate the overall average time\n",
    "overall_average_time = (average_time_mod1output + average_time_mod2output + average_time_mod3output) / 3\n",
    "\n",
    "# Create a DataFrame to store the averages\n",
    "df_avg_metrics = pd.DataFrame([avg_metrics1, avg_metrics2, avg_metrics3])\n",
    "\n",
    "# Add a new column for average time\n",
    "df_avg_metrics['Average Time'] = [average_time_mod1output, average_time_mod2output, average_time_mod3output]\n",
    "\n",
    "# Add a new row displaying overall average\n",
    "df_avg_metrics.loc['Overall Average'] = df_avg_metrics.mean()\n",
    "\n",
    "# Add a new row for overall average time\n",
    "df_avg_metrics.loc['Overall Average Time'] = overall_average_time\n",
    "\n",
    "# Rename the index to include model names\n",
    "df_avg_metrics.index = ['Model 1', 'Model 2', 'Model 3', 'Overall Average', 'Overall Average Time']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ec726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bfc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
