{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b497daa8",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: MD Alvee Rohan - U3235512 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Chain Of Thought </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b02fc6f",
   "metadata": {},
   "source": [
    "### Revised Prompt for COT\n",
    "\n",
    "### BIG-bench\\\\ c233\n",
    "\n",
    "We want to create a function that can adapt the concept of finding specific occurrences to track element counts while considering their order of appearance in the list. Let's think step by step: \n",
    "\n",
    "1. Initialize Dictionary: We need an empty dictionary to store element counts while keeping the order of appearance. This is similar to finding the first occurrence of a character.\n",
    "\n",
    "2. Iterate Through List: We need to process each element in the list, similar to iterating through the string to find characters.\n",
    "\n",
    "3. Check for Existing Element:  For the current element, check if it already exists as a key in the dictionary. This is equivalent to checking if the target character has already been encountered.\n",
    "\n",
    "4. Update Count (Existing Element): If the element exists (like a character found before), it's not the first occurrence. We can increment the count associated with that key in the dictionary, similar to keeping track of how many times a character has appeared.\n",
    "\n",
    "5. Add New Element (First Occurrence): If the element doesn't exist in the dictionary (like a new character), it's the first time we encounter it. We should add a new key-value pair to the dictionary: the element as the key and 1 (initial occurrence) as the value, similar to marking the first occurrence of a character.\n",
    "\n",
    "6. Return Dictionary: After iterating through the entire list, the dictionary will contain unique elements as keys and their corresponding counts while maintaining the order of appearance, similar to tracking all character occurrences in the order they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74926e94-f52f-4864-ae07-c808adc3da69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.00896\n",
      "Execution time Model 1 Output 2: 0.01152\n",
      "Execution time Model 1 Output 3: 0.01310\n",
      "Average Execution Time for Model 1 Output: 0.01119\n",
      "Expected Output:  [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
      "Actual Output:  [{14: 1, 25: 2, 13: 2, 20: 2, 55: 1}, {35: 4, 13: 5}, {95: 5}]\n",
      "   Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0        0.0     0.0         0       0.0            0\n",
      "1        0.0     0.0         0       0.0            0\n",
      "2        0.0     0.0         0       0.0            0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Function Generation - COT\n",
    "# Model 1\n",
    "\n",
    "import timeit\n",
    "\n",
    "\n",
    "def track_element_counts_model1(lst):\n",
    "    # Step 1: Initialize Dictionary\n",
    "    element_counts = {}\n",
    "    \n",
    "    # Step 2: Iterate Through List\n",
    "    for element in lst:\n",
    "        # Step 3: Check for Existing Element\n",
    "        if element in element_counts:\n",
    "            # Step 4: Update Count (Existing Element)\n",
    "            element_counts[element] += 1\n",
    "        else:\n",
    "            # Step 5: Add New Element (First Occurrence)\n",
    "            element_counts[element] = 1\n",
    "    \n",
    "    # Step 6: Return Dictionary\n",
    "    return element_counts\n",
    "\n",
    "\n",
    "mod1_input1 = [14, 25, 13, 25, 20, 13, 55, 20] #  [1, 2, 2, 2, 1]\n",
    "mod1_input2 = [35, 13, 35, 13, 35, 13, 35, 13, 13] # [4, 5]\n",
    "mod1_input3 = [95, 95, 95, 95, 95] # [5]\n",
    "\n",
    "mod1_output1 = track_element_counts_model1(mod1_input1)\n",
    "mod1_output2 = track_element_counts_model1(mod1_input2)\n",
    "mod1_output3 = track_element_counts_model1(mod1_input3)\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod1_output1 = timeit.timeit(lambda: track_element_counts_model1(mod1_input1), number=10000)\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "execution_time_mod1_output2 = timeit.timeit(lambda: track_element_counts_model1(mod1_input2), number=10000)\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "execution_time_mod1_output3 = timeit.timeit(lambda: track_element_counts_model1(mod1_input3), number=10000)\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "import pandas as pd\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    true_positives = sum(1 for num in actual_output if num in expected_output)\n",
    "    false_positives = sum(1 for num in actual_output if num not in expected_output)\n",
    "    false_negatives = sum(1 for num in expected_output if num not in actual_output)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for num in actual_output if num in expected_output) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "expected_outputs_c233 = [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
    "actual_outputs_mod1c233 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs_c233)\n",
    "print(\"Actual Output: \", actual_outputs_mod1c233)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs_c233, actual_outputs_mod1c233)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e3f012-0242-45c0-84a8-891700028efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.03188\n",
      "Execution time Model 2 Output 2: 0.04604\n",
      "Execution time Model 2 Output 3: 0.03353\n",
      "Average Execution Time for Model 2 Output: 0.03715\n",
      "Expected Output:  [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
      "Actual Output:  [[1, 2, 2, 2, 2, 2, 1, 2], [4, 5, 4, 5, 4, 5, 4, 5, 5], [5, 5, 5, 5, 5]]\n",
      "   Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0        1.0     1.0       1.0  0.500000            0\n",
      "1        1.0     1.0       1.0  0.222222            0\n",
      "2        1.0     1.0       1.0  0.200000            0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Function Generation - COT\n",
    "# Model 2\n",
    "\n",
    "from collections import Counter\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "def track_element_counts_model2(lst):\n",
    "    # Use Counter to count occurrences\n",
    "    counts = Counter(lst)\n",
    "    # Maintain order of appearance using list comprehension\n",
    "    ordered_counts = [counts[elem] for elem in lst if elem in counts]\n",
    "    return ordered_counts\n",
    "\n",
    "mod2_input1 = [14, 25, 13, 25, 20, 13, 55, 20] #  [1, 2, 2, 2, 1]\n",
    "mod2_input2 = [35, 13, 35, 13, 35, 13, 35, 13, 13] # [4, 5]\n",
    "mod2_input3 = [95, 95, 95, 95, 95] # [5]\n",
    "\n",
    "mod2_output1 = track_element_counts_model2(mod2_input1)\n",
    "mod2_output2 = track_element_counts_model2(mod2_input2)\n",
    "mod2_output3 = track_element_counts_model2(mod2_input3)\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod2_output1 = timeit.timeit(lambda: track_element_counts_model2(mod2_input1), number=10000)\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "execution_time_mod2_output2 = timeit.timeit(lambda: track_element_counts_model2(mod2_input2), number=10000)\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "execution_time_mod2_output3 = timeit.timeit(lambda: track_element_counts_model2(mod2_input3), number=10000)\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    \n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "    false_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 1 and actual_output[i] == 0)\n",
    "    false_negatives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 0 and actual_output[i] == 1)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs_c233 = [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
    "actual_outputs_mod2c233 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs_c233)\n",
    "print(\"Actual Output: \", actual_outputs_mod2c233)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs_c233, actual_outputs_mod2c233)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9b2bc-50ec-4cec-b807-64035c23a994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8753aa9e-9dfc-43b9-97bf-2101166d2eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.10205\n",
      "Execution time Model 3 Output 2: 0.08201\n",
      "Execution time Model 3 Output 3: 0.08694\n",
      "Average Execution Time for Model 3 Output: 0.09033\n",
      "Expected Output:  [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
      "Actual Output:  [[1, 2, 2, 2, 2, 2, 1, 2], [4, 5, 4, 5, 4, 5, 4, 5, 5], [5, 5, 5, 5, 5]]\n",
      "   Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0        1.0     1.0       1.0  0.500000            0\n",
      "1        1.0     1.0       1.0  0.222222            0\n",
      "2        1.0     1.0       1.0  0.200000            0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Function Generation - COT\n",
    "# Model 3\n",
    "\n",
    "import timeit\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "def track_element_counts_model3(lst):\n",
    "    # Use Counter to count occurrences\n",
    "    counts = Counter(lst)\n",
    "    # Maintain order of appearance using list comprehension\n",
    "    ordered_counts = [counts[elem] for elem in lst if elem in counts]\n",
    "    return ordered_counts\n",
    "\n",
    "\n",
    "mod3_input1 = [14, 25, 13, 25, 20, 13, 55, 20] #  [1, 2, 2, 2, 1]\n",
    "mod3_input2 = [35, 13, 35, 13, 35, 13, 35, 13, 13] # [4, 5]\n",
    "mod3_input3 = [95, 95, 95, 95, 95] # [5]\n",
    "\n",
    "mod3_output1 = track_element_counts_model3(mod3_input1)\n",
    "mod3_output2 = track_element_counts_model3(mod3_input2)\n",
    "mod3_output3 = track_element_counts_model3(mod3_input3)\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod3_output1 = timeit.timeit(lambda: track_element_counts_model3(mod3_input1), number=10000)\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "execution_time_mod3_output2 = timeit.timeit(lambda: track_element_counts_model3(mod3_input2), number=10000)\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "execution_time_mod3_output3 = timeit.timeit(lambda: track_element_counts_model3(mod3_input3), number=10000)\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output3, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "import pandas as pd\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    \n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "    false_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 1 and actual_output[i] == 0)\n",
    "    false_negatives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 0 and actual_output[i] == 1)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs_c233 = [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
    "actual_outputs_mod2c233 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs_c233)\n",
    "print(\"Actual Output: \", actual_outputs_mod2c233)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs_c233, actual_outputs_mod2c233)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192e0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
