{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71c150c",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: Pauline Armamento - U3246782 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Few-shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752494d",
   "metadata": {},
   "source": [
    "### HumanEval\n",
    "\n",
    "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models.\n",
    "https://huggingface.co/datasets/openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d534a",
   "metadata": {},
   "source": [
    "# Task ID 153\n",
    "Prompt: Write a Python function that takes the input of class name and list of extensions. Find the extension with the highest strength, calculated by the difference between uppercase and lowercase letters. Join the class name with the strongest extension using a period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eff614",
   "metadata": {},
   "source": [
    "### Revised Prompt for Few Shot\n",
    "Prompt: write a Python code snippet that does not use a function or function definition that finds the strongest extension to load additional classes for a given class name. The strength of each extension is determined by the difference between the number of uppercase and lowercase letters in its name. If there are multiple extensions with the same strength, choose the one that appears first in the list. Return the class name concatenated with the strongest extension in the format \"\"ClassName.StrongestExtensionName\"\".\n",
    "\n",
    "Example:\n",
    "   Input: Class name: 'Slices', Extensions: ['SErviNGSliCes', 'Cheese', 'StuFfed']\n",
    "   Output: 'Slices.SErviNGSliCes'\n",
    "\n",
    "   Input: Class name: 'my_class', Extensions: ['AA', 'Be', 'CC']\n",
    "   Output: 'my_class.AA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc15beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: HumanEval/153\n",
      "Prompt: \n",
      "def Strongest_Extension(class_name, extensions):\n",
      "    \"\"\"You will be given the name of a class (a string) and a list of extensions.\n",
      "    The extensions are to be used to load additional classes to the class. The\n",
      "    strength of the extension is as follows: Let CAP be the number of the uppercase\n",
      "    letters in the extension's name, and let SM be the number of lowercase letters \n",
      "    in the extension's name, the strength is given by the fraction CAP - SM. \n",
      "    You should find the strongest extension and return a string in this \n",
      "    format: ClassName.StrongestExtensionName.\n",
      "    If there are two or more extensions with the same strength, you should\n",
      "    choose the one that comes first in the list.\n",
      "    For example, if you are given \"Slices\" as the class and a list of the\n",
      "    extensions: ['SErviNGSliCes', 'Cheese', 'StuFfed'] then you should\n",
      "    return 'Slices.SErviNGSliCes' since 'SErviNGSliCes' is the strongest extension \n",
      "    (its strength is -1).\n",
      "    Example:\n",
      "    for Strongest_Extension('my_class', ['AA', 'Be', 'CC']) == 'my_class.AA'\n",
      "    \"\"\"\n",
      "\n",
      "Canonical Solution:     strong = extensions[0]\n",
      "    my_val = len([x for x in extensions[0] if x.isalpha() and x.isupper()]) - len([x for x in extensions[0] if x.isalpha() and x.islower()])\n",
      "    for s in extensions:\n",
      "        val = len([x for x in s if x.isalpha() and x.isupper()]) - len([x for x in s if x.isalpha() and x.islower()])\n",
      "        if val > my_val:\n",
      "            strong = s\n",
      "            my_val = val\n",
      "\n",
      "    ans = class_name + \".\" + strong\n",
      "    return ans\n",
      "\n",
      "\n",
      "Test Data: def check(candidate):\n",
      "\n",
      "    # Check some simple cases\n",
      "    assert candidate('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']) == 'Watashi.eIGHt8OKe'\n",
      "    assert candidate('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) == 'Boku123.YEs.WeCaNe'\n",
      "    assert candidate('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) == '__YESIMHERE.NuLl__'\n",
      "    assert candidate('K', ['Ta', 'TAR', 't234An', 'cosSo']) == 'K.TAR'\n",
      "    assert candidate('__HAHA', ['Tab', '123', '781345', '-_-']) == '__HAHA.123'\n",
      "    assert candidate('YameRore', ['HhAas', 'okIWILL123', 'WorkOut', 'Fails', '-_-']) == 'YameRore.okIWILL123'\n",
      "    assert candidate('finNNalLLly', ['Die', 'NowW', 'Wow', 'WoW']) == 'finNNalLLly.WoW'\n",
      "\n",
      "    # Check some edge cases that are easy to work out by hand.\n",
      "    assert candidate('_', ['Bb', '91245']) == '_.Bb'\n",
      "    assert candidate('Sp', ['671235', 'Bb']) == 'Sp.671235'\n",
      "    \n",
      "\n",
      "Entry Point: Strongest_Extension\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "# Initialize Task Index\n",
    "task_index = 153\n",
    "task_153 = dataset[\"test\"][task_index]\n",
    "\n",
    "# Access Task Details\n",
    "task_id = task_153[\"task_id\"]\n",
    "prompt = task_153[\"prompt\"]\n",
    "canonical_solution = task_153[\"canonical_solution\"]\n",
    "test_data = task_153[\"test\"]\n",
    "entry_point = task_153[\"entry_point\"]\n",
    "\n",
    "# Display Task Details\n",
    "print(f\"Task ID: {task_id}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Canonical Solution: {canonical_solution}\")\n",
    "print(f\"Test Data: {test_data}\")\n",
    "print(f\"Entry Point: {entry_point}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd7cb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.04247\n",
      "Execution time Model 1 Output 2: 0.05932\n",
      "Execution time Model 1 Output 3: 0.07718\n",
      "Average Execution Time for Model 1 Output: 0.05966\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 1 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function Generation - Few Shot\n",
    "# Model 1\n",
    "\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "def Strongest_Extension1(class_name, extensions):\n",
    "    strongest_extension = None\n",
    "    max_strength = float('-inf')\n",
    "\n",
    "    for extension in extensions:\n",
    "        cap_count = sum(1 for char in extension if char.isupper())\n",
    "        sm_count = sum(1 for char in extension if char.islower())\n",
    "        strength = cap_count - sm_count\n",
    "\n",
    "        if strength > max_strength:\n",
    "            strongest_extension = extension\n",
    "            max_strength = strength\n",
    "\n",
    "    return f\"{class_name}.{strongest_extension}\"\n",
    "\n",
    "# Test cases\n",
    "mod1_output1 = Strongest_Extension1('Watashi', ['tEN', 'niNE', 'eIGHt8OKe'])  # Output: 'Watashi.eIGHt8OKe'\n",
    "mod1_output2 = Strongest_Extension1('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) # Output: 'Boku123.YEs.WeCaNe'\n",
    "mod1_output3 = Strongest_Extension1('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) # Output: '__YESIMHERE.NuLl__'\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod1_output1 = timeit.timeit(lambda: Strongest_Extension1('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']), number=10000)\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "execution_time_mod1_output2 = timeit.timeit(lambda: Strongest_Extension1('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']), number=10000)\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "\n",
    "execution_time_mod1_output3 = timeit.timeit(lambda: Strongest_Extension1('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']), number=10000)\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20cb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.04788\n",
      "Execution time Model 2 Output 2: 0.06079\n",
      "Execution time Model 2 Output 3: 0.07619\n",
      "Average Execution Time for Model 2 Output: 0.06162\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 2 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function Generation - Few Shot\n",
    "# Model 2\n",
    "\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "def Strongest_Extension2(class_name, extensions):\n",
    "    # Define a custom key function to determine strength\n",
    "    def strength_key(extension):\n",
    "        cap_count = sum(1 for char in extension if char.isupper())\n",
    "        sm_count = sum(1 for char in extension if char.islower())\n",
    "        return cap_count - sm_count\n",
    "    \n",
    "    # Find the extension with maximum strength based on the custom key\n",
    "    strongest_extension = max(extensions, key=strength_key)\n",
    "    \n",
    "    return f\"{class_name}.{strongest_extension}\"\n",
    "\n",
    "# Test cases\n",
    "mod2_output1 = Strongest_Extension2('Watashi', ['tEN', 'niNE', 'eIGHt8OKe'])  # Output: 'Watashi.eIGHt8OKe'\n",
    "mod2_output2 = Strongest_Extension2('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) # Output: 'Boku123.YEs.WeCaNe'\n",
    "mod2_output3 = Strongest_Extension2('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) # Output: '__YESIMHERE.NuLl__'\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod2_output1 = timeit.timeit(lambda: Strongest_Extension2('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']), number=10000)\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "execution_time_mod2_output2 = timeit.timeit(lambda: Strongest_Extension2('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']), number=10000)\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "execution_time_mod2_output3 = timeit.timeit(lambda: Strongest_Extension2('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']), number=10000)\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod2 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod2)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod2)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics2_new = pd.concat([df_metrics2_new, avg_metrics2.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec8f45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.05157\n",
      "Execution time Model 3 Output 2: 0.07193\n",
      "Execution time Model 3 Output 3: 0.08940\n",
      "Average Execution Time for Model 3 Output: 0.07097\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 3 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function Generation - Few Shot\n",
    "# Model 3\n",
    "\n",
    "def Strongest_Extension3(class_name, extensions):\n",
    "    # Calculate strengths for all extensions\n",
    "    strengths = [sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower()) for ext in extensions]\n",
    "\n",
    "    # Find the index of the extension with maximum strength\n",
    "    max_index = max(range(len(extensions)), key=lambda i: strengths[i])\n",
    "\n",
    "    # Return the strongest extension\n",
    "    return f\"{class_name}.{extensions[max_index]}\"\n",
    "\n",
    "# Test cases\n",
    "mod3_output1 = Strongest_Extension3('Watashi', ['tEN', 'niNE', 'eIGHt8OKe'])  # Output: 'Watashi.eIGHt8OKe'\n",
    "mod3_output2 = Strongest_Extension3('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) # Output: 'Boku123.YEs.WeCaNe'\n",
    "mod3_output3 = Strongest_Extension3('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) # Output: '__YESIMHERE.NuLl__'\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod3_output1 = timeit.timeit(lambda: Strongest_Extension3('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']), number=10000)\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "execution_time_mod3_output2 = timeit.timeit(lambda: Strongest_Extension3('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']), number=10000)\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "execution_time_mod3_output3 = timeit.timeit(lambda: Strongest_Extension3('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']), number=10000)\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output3, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod3 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod3)\n",
    "\n",
    "metrics3 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod3)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics3_new = pd.DataFrame(metrics3)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "avg_metrics3.name = 'Model 3 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics3_new = pd.concat([df_metrics3_new, avg_metrics3.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics3_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ec7a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Precision    Recall  F1 Score  Accuracy  Exact Match  \\\n",
      "Model 1                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Model 2                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Model 3                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Overall Average        1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Overall Average Time   0.063427  0.063427  0.063427  0.063427     0.063427   \n",
      "\n",
      "                      Average Time  \n",
      "Model 1                   0.059656  \n",
      "Model 2                   0.059656  \n",
      "Model 3                   0.070968  \n",
      "Overall Average           0.063427  \n",
      "Overall Average Time      0.063427  \n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Overall Metrics\n",
    "\n",
    "# Calculate the averages for each set of metrics\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "\n",
    "# Calculate the overall average time\n",
    "overall_average_time = (average_time_mod1output + average_time_mod2output + average_time_mod3output) / 3\n",
    "\n",
    "# Create a DataFrame to store the averages\n",
    "df_avg_metrics = pd.DataFrame([avg_metrics1, avg_metrics2, avg_metrics3])\n",
    "\n",
    "# Add a new column for average time\n",
    "df_avg_metrics['Average Time'] = [average_time_mod1output, average_time_mod2output, average_time_mod3output]\n",
    "\n",
    "# Add a new row displaying overall average\n",
    "df_avg_metrics.loc['Overall Average'] = df_avg_metrics.mean()\n",
    "\n",
    "# Add a new row for overall average time\n",
    "df_avg_metrics.loc['Overall Average Time'] = overall_average_time\n",
    "\n",
    "# Rename the index to include model names\n",
    "df_avg_metrics.index = ['Model 1', 'Model 2', 'Model 3', 'Overall Average', 'Overall Average Time']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ec726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bfc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
