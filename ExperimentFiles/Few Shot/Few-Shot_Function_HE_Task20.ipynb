{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71c150c",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: Pauline Armamento - U3246782 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Few-shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752494d",
   "metadata": {},
   "source": [
    "### HumanEval\n",
    "\n",
    "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models.\n",
    "https://huggingface.co/datasets/openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c89c5",
   "metadata": {},
   "source": [
    "# Task ID 20\n",
    "Prompt: Write a Python function that takes a list of numbers and  returns two numbers that are closest to each other in  the order of smallest and largest number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685e453",
   "metadata": {},
   "source": [
    "### Revised Prompt for Few Shot\n",
    "Prompt: You're given a list of numbers, which contains at least two elements. Your task is to write a python code snippet without using function and function definition that selects and return two numbers from the list that are closest to each other. Return them in ascending order (smaller number first, larger number second).\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: [1.0, 2.0, 3.0, 4.0, 5.0, 2.2]\n",
    "Output: (2.0, 2.2)\n",
    "\n",
    "\n",
    "Input: [1.0, 2.0, 3.0, 4.0, 5.0, 2.0]\n",
    "Output: (2.0, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc15beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: HumanEval/20\n",
      "Prompt: from typing import List, Tuple\n",
      "\n",
      "\n",
      "def find_closest_elements(numbers: List[float]) -> Tuple[float, float]:\n",
      "    \"\"\" From a supplied list of numbers (of length at least two) select and return two that are the closest to each\n",
      "    other and return them in order (smaller number, larger number).\n",
      "    >>> find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.2])\n",
      "    (2.0, 2.2)\n",
      "    >>> find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0])\n",
      "    (2.0, 2.0)\n",
      "    \"\"\"\n",
      "\n",
      "Canonical Solution:     closest_pair = None\n",
      "    distance = None\n",
      "\n",
      "    for idx, elem in enumerate(numbers):\n",
      "        for idx2, elem2 in enumerate(numbers):\n",
      "            if idx != idx2:\n",
      "                if distance is None:\n",
      "                    distance = abs(elem - elem2)\n",
      "                    closest_pair = tuple(sorted([elem, elem2]))\n",
      "                else:\n",
      "                    new_distance = abs(elem - elem2)\n",
      "                    if new_distance < distance:\n",
      "                        distance = new_distance\n",
      "                        closest_pair = tuple(sorted([elem, elem2]))\n",
      "\n",
      "    return closest_pair\n",
      "\n",
      "Test Data: \n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]) == (3.9, 4.0)\n",
      "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0]) == (5.0, 5.9)\n",
      "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.2]) == (2.0, 2.2)\n",
      "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0]) == (2.0, 2.0)\n",
      "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1]) == (2.2, 3.1)\n",
      "\n",
      "\n",
      "Entry Point: find_closest_elements\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "# Initialize Task Index\n",
    "task_index = 20\n",
    "task_153 = dataset[\"test\"][task_index]\n",
    "\n",
    "# Access Task Details\n",
    "task_id = task_153[\"task_id\"]\n",
    "prompt = task_153[\"prompt\"]\n",
    "canonical_solution = task_153[\"canonical_solution\"]\n",
    "test_data = task_153[\"test\"]\n",
    "entry_point = task_153[\"entry_point\"]\n",
    "\n",
    "# Display Task Details\n",
    "print(f\"Task ID: {task_id}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Canonical Solution: {canonical_solution}\")\n",
    "print(f\"Test Data: {test_data}\")\n",
    "print(f\"Entry Point: {entry_point}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd7cb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.01706\n",
      "Execution time Model 1 Output 2: 0.01127\n",
      "Execution time Model 1 Output 3: 0.01417\n",
      "Average Execution Time for Model 1 Output: 0.01417\n",
      "Expected Output:  [(3.9, 4.0), (5.0, 5.9), (2.2, 3.1)]\n",
      "Actual Output:  [(3.9, 4.0), (5.0, 5.9), (2.2, 3.1)]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 1 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/20\n",
    "# Function Generation - Few Shot\n",
    "# Model 1\n",
    "\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "def find_closest_elements1(numbers: List[float]) -> Tuple[float, float]:\n",
    "    numbers.sort()  # Sort the numbers in ascending order\n",
    "    min_diff = float('inf')  # Initialize min_diff with positive infinity\n",
    "    closest_pair = ()  # Initialize closest_pair as an empty tuple\n",
    "    \n",
    "    # Iterate through the list of numbers to find the pair with the smallest difference\n",
    "    for i in range(len(numbers) - 1):\n",
    "        diff = numbers[i + 1] - numbers[i]\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            closest_pair = (numbers[i], numbers[i + 1])\n",
    "    \n",
    "    return closest_pair\n",
    "\n",
    "\n",
    "\n",
    "# Test cases\n",
    "mod1_output1 = find_closest_elements1([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]) # (3.9, 4.0)\n",
    "mod1_output2 = find_closest_elements1([1.0, 2.0, 5.9, 4.0, 5.0]) # (5.0, 5.9)\n",
    "mod1_output3 = find_closest_elements1([1.1, 2.2, 3.1, 4.1, 5.1]) # (2.2, 3.1)\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod1_output1 = timeit.timeit(lambda: find_closest_elements1([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]), number=10000)\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "execution_time_mod1_output2 = timeit.timeit(lambda: find_closest_elements1([1.0, 2.0, 5.9, 4.0, 5.0]), number=10000)\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "\n",
    "execution_time_mod1_output3 = timeit.timeit(lambda: find_closest_elements1([1.1, 2.2, 3.1, 4.1, 5.1]), number=10000)\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [(3.9, 4.0), (5.0, 5.9), (2.2, 3.1)]\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b20cb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.03660\n",
      "Execution time Model 2 Output 2: 0.02487\n",
      "Execution time Model 2 Output 3: 0.02526\n",
      "Average Execution Time for Model 2 Output: 0.02891\n",
      "Expected Output:  [(3.9, 4.0), (5.0, 5.9), (2.2, 3.1)]\n",
      "Actual Output:  [(3.9, 4.0), (5.9, 5.0), (2.2, 3.1)]\n",
      "                 Precision    Recall  F1 Score  Accuracy  Exact Match\n",
      "0                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "1                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "2                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "Model 2 Average   0.666667  0.666667  0.666667  0.666667     0.666667\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/95\n",
    "# Function Generation - Few Shot\n",
    "# Model 2\n",
    "\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "def find_closest_elements2(numbers: List[float]) -> Tuple[float, float]:\n",
    "    closest_pair = (numbers[0], numbers[1])  # Initialize closest_pair with the first two elements\n",
    "    min_diff = abs(numbers[1] - numbers[0])  # Initialize min_diff with the absolute difference between the first two elements\n",
    "    \n",
    "    # Iterate through all possible pairs of numbers\n",
    "    for i in range(len(numbers)):\n",
    "        for j in range(i+1, len(numbers)):\n",
    "            diff = abs(numbers[j] - numbers[i])\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                closest_pair = (numbers[i], numbers[j])\n",
    "    \n",
    "    return closest_pair\n",
    "\n",
    "\n",
    "# Test cases\n",
    "mod2_output1 = find_closest_elements2([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]) # (3.9, 4.0)\n",
    "mod2_output2 = find_closest_elements2([1.0, 2.0, 5.9, 4.0, 5.0]) # (5.0, 5.9)\n",
    "mod2_output3 = find_closest_elements2([1.1, 2.2, 3.1, 4.1, 5.1]) # (2.2, 3.1)\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod2_output1 = timeit.timeit(lambda: find_closest_elements2([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]), number=10000)\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "execution_time_mod2_output2 = timeit.timeit(lambda: find_closest_elements2([1.0, 2.0, 5.9, 4.0, 5.0]), number=10000)\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "execution_time_mod2_output3 = timeit.timeit(lambda: find_closest_elements2([1.1, 2.2, 3.1, 4.1, 5.1]), number=10000)\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [(3.9, 4.0), (5.0, 5.9), (2.2, 3.1)]\n",
    "actual_outputs_mod2 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod2)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod2)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics2_new = pd.concat([df_metrics2_new, avg_metrics2.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec8f45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.03992\n",
      "Execution time Model 3 Output 2: 0.02647\n",
      "Execution time Model 3 Output 3: 0.02675\n",
      "Average Execution Time for Model 3 Output: 0.03105\n",
      "Expected Output:  [(3.9, 4.0), (5.0, 5.9), (2.2, 3.1)]\n",
      "Actual Output:  [(3.9, 4.0), (5.9, 5.0), (2.2, 3.1)]\n",
      "                 Precision    Recall  F1 Score  Accuracy  Exact Match\n",
      "0                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "1                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "2                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "Model 3 Average   0.666667  0.666667  0.666667  0.666667     0.666667\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/20\n",
    "# Function Generation - Few Shot\n",
    "# Model 3\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "def find_closest_elements3(numbers: List[float]) -> Tuple[float, float]:\n",
    "    closest_pair = (numbers[0], numbers[1])  # Initialize closest_pair with the first two elements\n",
    "    min_diff = abs(numbers[1] - numbers[0])  # Initialize min_diff with the absolute difference between the first two elements\n",
    "    \n",
    "    # Iterate through all possible pairs of numbers\n",
    "    for i in range(len(numbers)):\n",
    "        for j in range(i+1, len(numbers)):\n",
    "            diff = abs(numbers[j] - numbers[i])\n",
    "            # If the current pair has a smaller difference than the current closest pair, update closest_pair and min_diff\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                closest_pair = (numbers[i], numbers[j])\n",
    "            # If the difference is 0, we've found the closest possible pair, so we can return immediately\n",
    "            if min_diff == 0:\n",
    "                return closest_pair\n",
    "    \n",
    "    return closest_pair\n",
    "\n",
    "\n",
    "# Test cases\n",
    "mod3_output1 = find_closest_elements3([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]) # (3.9, 4.0)\n",
    "mod3_output2 = find_closest_elements3([1.0, 2.0, 5.9, 4.0, 5.0]) # (5.0, 5.9)\n",
    "mod3_output3 = find_closest_elements3([1.1, 2.2, 3.1, 4.1, 5.1]) # (2.2, 3.1)\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod3_output1 = timeit.timeit(lambda: find_closest_elements3([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]), number=10000)\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "execution_time_mod3_output2 = timeit.timeit(lambda: find_closest_elements3([1.0, 2.0, 5.9, 4.0, 5.0]), number=10000)\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "execution_time_mod3_output3 = timeit.timeit(lambda: find_closest_elements3([1.1, 2.2, 3.1, 4.1, 5.1]), number=10000)\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output3, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [(3.9, 4.0), (5.0, 5.9), (2.2, 3.1)]\n",
    "actual_outputs_mod3 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod3)\n",
    "\n",
    "metrics3 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod3)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics3_new = pd.DataFrame(metrics3)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "avg_metrics3.name = 'Model 3 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics3_new = pd.concat([df_metrics3_new, avg_metrics3.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics3_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec7a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Precision    Recall  F1 Score  Accuracy  Exact Match  \\\n",
      "Model 1                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Model 2                0.666667  0.666667  0.666667  0.666667     0.666667   \n",
      "Model 3                0.666667  0.666667  0.666667  0.666667     0.666667   \n",
      "Overall Average        0.777778  0.777778  0.777778  0.777778     0.777778   \n",
      "Overall Average Time   0.024709  0.024709  0.024709  0.024709     0.024709   \n",
      "\n",
      "                      Average Time  \n",
      "Model 1                   0.014169  \n",
      "Model 2                   0.028911  \n",
      "Model 3                   0.031049  \n",
      "Overall Average           0.024709  \n",
      "Overall Average Time      0.024709  \n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/67\n",
    "# Overall Metrics\n",
    "\n",
    "# Calculate the averages for each set of metrics\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "\n",
    "# Calculate the overall average time\n",
    "overall_average_time = (average_time_mod1output + average_time_mod2output + average_time_mod3output) / 3\n",
    "\n",
    "# Create a DataFrame to store the averages\n",
    "df_avg_metrics = pd.DataFrame([avg_metrics1, avg_metrics2, avg_metrics3])\n",
    "\n",
    "# Add a new column for average time\n",
    "df_avg_metrics['Average Time'] = [average_time_mod1output, average_time_mod2output, average_time_mod3output]\n",
    "\n",
    "# Add a new row displaying overall average\n",
    "df_avg_metrics.loc['Overall Average'] = df_avg_metrics.mean()\n",
    "\n",
    "# Add a new row for overall average time\n",
    "df_avg_metrics.loc['Overall Average Time'] = overall_average_time\n",
    "\n",
    "# Rename the index to include model names\n",
    "df_avg_metrics.index = ['Model 1', 'Model 2', 'Model 3', 'Overall Average', 'Overall Average Time']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ec726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bfc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
