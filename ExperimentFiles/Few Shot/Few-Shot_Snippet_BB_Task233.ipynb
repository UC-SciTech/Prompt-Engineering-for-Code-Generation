{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e9e291",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: MD Alvee Rohan - U3235512 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Few Shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0009c4",
   "metadata": {},
   "source": [
    "### Revised Prompt for Few Shot\n",
    "\n",
    "### BIG-bench\\\\ c233\n",
    "\n",
    "Write a python code snippet without using a function or function definition  that takes a list of natural numbers and returns a dictionary where the keys are unique elements from the list, and the values are the number of times each element appears, in the order they first appear in the list.\n",
    "\n",
    "Example:\n",
    "\"\"input\"\": \"\"[41, 65, 65, 65, 41]\"\",\n",
    "\"\"target\"\": \"\"[2, 3]\"\"\n",
    "\n",
    " \"\"input\"\": \"\"[12, 12, 12, 12, 12, 12]\"\",\n",
    "\"\"target\"\": \"\"[6]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afb170d6-1b61-4eb4-9b46-0765d7566021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 2, 1]\n",
      "Execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Code Snippet - FS\n",
    "# Model 1\n",
    "# Sample 1\n",
    "\n",
    "import time\n",
    "# Record the starting time\n",
    "start_time_mol1_output1 = time.time()\n",
    "\n",
    "# Input list\n",
    "input_list = [14, 25, 13, 25, 20, 13, 55, 20]\n",
    "\n",
    "# Dictionary to store frequencies\n",
    "frequency_dict = {}\n",
    "\n",
    "# Iterate through the list\n",
    "for num in input_list:\n",
    "    # If the number is not in the dictionary, add it with a count of 1\n",
    "    if num not in frequency_dict:\n",
    "        frequency_dict[num] = 1\n",
    "    # If the number is already in the dictionary, increment its count\n",
    "    else:\n",
    "        frequency_dict[num] += 1\n",
    "\n",
    "# Extract unique elements in the order they first appear\n",
    "unique_elements = []\n",
    "for num in input_list:\n",
    "    if num not in unique_elements:\n",
    "        unique_elements.append(num)\n",
    "\n",
    "# Create the target list of frequencies in the order of appearance\n",
    "target_list = [frequency_dict[num] for num in unique_elements]\n",
    "\n",
    "print(target_list)\n",
    "\n",
    "\n",
    "\n",
    "# Assign the combined list to mod1_output1\n",
    "mod1_output1 = target_list\n",
    "# Record the ending time\n",
    "end_time_mol1_output1 = time.time()\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(f\"Execution time: {execution_time_mod1_output1} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7f8d2d8-f33d-41a1-bcb0-20f0e40aed3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]\n",
      "Execution time: 0.007914543151855469 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Code Snippet - FS\n",
    "# Model 1\n",
    "# Sample 2\n",
    "\n",
    "import time\n",
    "# Record the starting time\n",
    "start_time_mol1_output2 = time.time()\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Input list\n",
    "input_list = [35, 13, 35, 13, 35, 13, 35, 13, 13]\n",
    "\n",
    "# Create an ordered dictionary to preserve the order of appearance\n",
    "frequency_dict = OrderedDict()\n",
    "\n",
    "# Iterate through the list\n",
    "[frequency_dict.setdefault(num, []).append(None) for num in input_list]\n",
    "\n",
    "# Create the target list of frequencies\n",
    "target_list = [len(frequency_dict[num]) for num in frequency_dict]\n",
    "\n",
    "print(target_list)\n",
    "\n",
    "\n",
    "# Assign the combined list to mod1_output1\n",
    "mod1_output2 = target_list\n",
    "# Record the ending time\n",
    "end_time_mol1_output2 = time.time()\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(f\"Execution time: {execution_time_mod1_output2} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8b89f3a-53bf-49e3-81c2-07a0a27a6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "Execution time: 0.001993894577026367 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Code Snippet - FS\n",
    "# Model 1\n",
    "# Sample 3\n",
    "\n",
    "import time\n",
    "# Record the starting time\n",
    "start_time_mol1_output3 = time.time()\n",
    "\n",
    "input_list = [95, 95, 95, 95, 95]\n",
    "# Create an ordered dictionary to preserve the order of appearance\n",
    "frequency_dict = OrderedDict()\n",
    "\n",
    "# Iterate through the list\n",
    "[frequency_dict.setdefault(num, []).append(None) for num in input_list]\n",
    "\n",
    "# Create the target list of frequencies\n",
    "target_list = [len(frequency_dict[num]) for num in frequency_dict]\n",
    "\n",
    "print(target_list)\n",
    "\n",
    "# Assign the combined list to mod1_output1\n",
    "mod1_output3 = target_list\n",
    "# Record the ending time\n",
    "end_time_mol1_output3 = time.time()\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(f\"Execution time: {execution_time_mod1_output3} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d914650f-fb72-4fab-8f66-ebb9f84b345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Output:  [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
      "Actual Output:  [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 1 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "## Model 1 summary\n",
    "import pandas as pd\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    # Calculate true positives\n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "\n",
    "    # Calculate false positives\n",
    "    false_positives = sum(1 for i in range(len(actual_output)) if actual_output[i] not in expected_output)\n",
    "\n",
    "    # Calculate false negatives\n",
    "    false_negatives = sum(1 for i in range(len(expected_output)) if expected_output[i] != actual_output[i])\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(1 for i in range(len(expected_output)) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# # Append the average row to the DataFrame\n",
    "# df_metrics1_new = df_metrics1_new.append(avg_metrics1)\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0443b9e8-46d5-4824-b304-4f1caeaf9d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 2, 1]\n",
      "Execution time: 0.0009961128234863281 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Code Snippet - FS\n",
    "# Model 2\n",
    "# Sample 1\n",
    "\n",
    "import time\n",
    "# Record the starting time\n",
    "start_time_mol2_output1 = time.time()\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Input list\n",
    "input_list = [14, 25, 13, 25, 20, 13, 55, 20]\n",
    "\n",
    "# Create an ordered dictionary to preserve the order of appearance\n",
    "frequency_dict = OrderedDict()\n",
    "\n",
    "# Iterate through the list\n",
    "[frequency_dict.setdefault(num, []).append(None) for num in input_list]\n",
    "\n",
    "# Create the target list of frequencies\n",
    "target_list = [len(frequency_dict[num]) for num in frequency_dict]\n",
    "\n",
    "print(target_list)\n",
    "\n",
    "\n",
    "# Assign the combined list to mod1_output1\n",
    "mod2_output1 = target_list\n",
    "# Record the ending time\n",
    "end_time_mol2_output1 = time.time()\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod2_output1 = end_time_mol2_output1 - start_time_mol2_output1\n",
    "print(f\"Execution time: {execution_time_mod2_output1} seconds\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e15b3a2-4d73-4caa-95f7-76be4aab3fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]\n",
      "Execution time: 0.0009975433349609375 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Code Snippet - FS\n",
    "# Model 1\n",
    "# Sample 2\n",
    "\n",
    "import time\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol2_output2 = time.time()\n",
    "# Input list of natural numbers\n",
    "input_list = [35, 13, 35, 13, 35, 13, 35, 13, 13]\n",
    "# Create an ordered dictionary to preserve the order of appearance\n",
    "frequency_dict = OrderedDict()\n",
    "\n",
    "# Iterate through the list\n",
    "[frequency_dict.setdefault(num, []).append(None) for num in input_list]\n",
    "\n",
    "# Create the target list of frequencies\n",
    "target_list = [len(frequency_dict[num]) for num in frequency_dict]\n",
    "\n",
    "print(target_list)\n",
    "\n",
    "# Assign the combined list to mod2_output2\n",
    "mod2_output2 = target_list\n",
    "# Record the ending time\n",
    "end_time_mol2_output2 = time.time()\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod2_output2 = end_time_mol2_output2 - start_time_mol2_output2\n",
    "print(f\"Execution time: {execution_time_mod2_output2} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d887d14-82b9-4aaf-a3a6-0b4f06a4b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "Execution time: 0.001993894577026367 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Code Snippet - FS\n",
    "# Model 1\n",
    "# Sample 3\n",
    "\n",
    "import time\n",
    "# Record the starting time\n",
    "start_time_mol2_output3 = time.time()\n",
    "# Input list of natural numbers\n",
    "input_list = [95, 95, 95, 95, 95]\n",
    "# Create an ordered dictionary to preserve the order of appearance\n",
    "frequency_dict = OrderedDict()\n",
    "\n",
    "# Iterate through the list\n",
    "[frequency_dict.setdefault(num, []).append(None) for num in input_list]\n",
    "\n",
    "# Create the target list of frequencies\n",
    "target_list = [len(frequency_dict[num]) for num in frequency_dict]\n",
    "\n",
    "print(target_list)\n",
    "# Record the ending time\n",
    "end_time_mol2_output3 = time.time()\n",
    "\n",
    "# Assign the combined list\n",
    "mod2_output3 = target_list\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod2_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(f\"Execution time: {execution_time_mod1_output3} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d8371b7-4113-4aa0-8f4d-f8101a893dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Output:  [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
      "Actual Output:  [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 1 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "## Model 2 summary\n",
    "import pandas as pd\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    # Calculate true positives\n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "\n",
    "    # Calculate false positives\n",
    "    false_positives = sum(1 for i in range(len(actual_output)) if actual_output[i] not in expected_output)\n",
    "\n",
    "    # Calculate false negatives\n",
    "    false_negatives = sum(1 for i in range(len(expected_output)) if expected_output[i] != actual_output[i])\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(1 for i in range(len(expected_output)) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# # Append the average row to the DataFrame\n",
    "# df_metrics1_new = df_metrics1_new.append(avg_metrics1)\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71204276-4278-4312-8a38-52230cd28431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Code Snippet - FS\n",
    "# Model 3\n",
    "# Sample 1\n",
    "\n",
    "import time\n",
    "# Record the starting time\n",
    "start_time_mol3_output1 = time.time()\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "# Input list of natural numbers\n",
    "input_list = [14, 25, 13, 25, 20, 13, 55, 20]\n",
    "\n",
    "# Use groupby to group identical elements together\n",
    "grouped = groupby(input_list)\n",
    "\n",
    "# Create the target list of frequencies\n",
    "target_list = [len(list(group)) for _, group in grouped]\n",
    "\n",
    "print(target_list)\n",
    "\n",
    "# Record the ending time\n",
    "end_time_mol3_output1 = time.time()\n",
    "\n",
    "\n",
    "mod3_output1 = target_list\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod3_output1 = end_time_mol3_output1 - start_time_mol3_output1\n",
    "print(f\"Execution time: {execution_time_mod3_output1} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69ba3304-8515-4e7c-ad32-68b2e8a0a5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 2]\n",
      "Execution time: 0.0009946823120117188 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Code Snippet - FS\n",
    "# Model 3\n",
    "# Sample 2\n",
    "\n",
    "import time\n",
    "# Record the starting time\n",
    "start_time_mol3_output2 = time.time()\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "# Input list of natural numbers\n",
    "input_list = [35, 13, 35, 13, 35, 13, 35, 13, 13]\n",
    "\n",
    "# Use groupby to group identical elements together\n",
    "grouped = groupby(input_list)\n",
    "\n",
    "# Create the target list of frequencies\n",
    "target_list = [len(list(group)) for _, group in grouped]\n",
    "print(target_list)\n",
    "\n",
    "# Assign the combined list to mod2_output2\n",
    "mod3_output2 = target_list\n",
    "# Record the ending time\n",
    "end_time_mol3_output2 = time.time()\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod3_output2 = end_time_mol3_output2 - start_time_mol3_output2\n",
    "print(f\"Execution time: {execution_time_mod3_output2} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4755d5f-15ec-4713-9d5b-c4b385e8f2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "Execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: BIG-bench/c233\n",
    "# Code Snippet - FS\n",
    "# Model 3\n",
    "# Sample 3\n",
    "\n",
    "import time\n",
    "# Record the starting time\n",
    "start_time_mol3_output3 = time.time()\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "# Input list of natural numbers\n",
    "input_list = [95, 95, 95, 95, 95]\n",
    "\n",
    "# Use groupby to group identical elements together\n",
    "grouped = groupby(input_list)\n",
    "\n",
    "# Create the target list of frequencies\n",
    "target_list = [len(list(group)) for _, group in grouped]\n",
    "print(target_list)\n",
    "\n",
    "# Record the ending time\n",
    "end_time_mol3_output3 = time.time()\n",
    "\n",
    "\n",
    "mod3_output3 = target_list\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod3_output3 = end_time_mol3_output3 - start_time_mol3_output3\n",
    "print(f\"Execution time: {execution_time_mod3_output3} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "875870aa-6acf-40c3-8fed-4b085cad34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Output:  [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
      "Actual Output:  [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 1 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "## Model 3 summary\n",
    "import pandas as pd\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    # Calculate true positives\n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "\n",
    "    # Calculate false positives\n",
    "    false_positives = sum(1 for i in range(len(actual_output)) if actual_output[i] not in expected_output)\n",
    "\n",
    "    # Calculate false negatives\n",
    "    false_negatives = sum(1 for i in range(len(expected_output)) if expected_output[i] != actual_output[i])\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(1 for i in range(len(expected_output)) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [[1, 2, 2, 2, 1], [4, 5], [5]]\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# # Append the average row to the DataFrame\n",
    "# df_metrics1_new = df_metrics1_new.append(avg_metrics1)\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ccd36-f2f7-4891-98b5-5600b0fe59f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
