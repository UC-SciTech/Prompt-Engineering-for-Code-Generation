{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71c150c",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: Pauline Armamento - U3246782 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Few-shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752494d",
   "metadata": {},
   "source": [
    "### HumanEval\n",
    "\n",
    "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models.\n",
    "https://huggingface.co/datasets/openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4ee8e",
   "metadata": {},
   "source": [
    "# Task ID 153\n",
    "Prompt: Write a Python function that takes the input of class name and list of extensions. Find the extension with the highest strength, calculated by the difference between uppercase and lowercase letters. Join the class name with the strongest extension using a period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359e988",
   "metadata": {},
   "source": [
    "### Revised Prompt for Few Shot\n",
    "Prompt: You will be given the name of a class (a string) and a list of extensions. The extensions are to be used to load additional classes to the class. The strength of the extension is as follows: Let CAP be the number of the uppercase  letters in the extension's name, and let SM be the number of lowercase letters in the extension's name, the strength is given by the fraction CAP - SM. You should find the strongest extension and return a string in this format: ClassName.StrongestExtensionName. If there are two or more extensions with the same strength, you should choose the one that comes first in the list.\n",
    "\n",
    "Example:\n",
    "    for Strongest_Extension(‘Slices’, ['SErviNGSliCes', 'Cheese', 'StuFfed’]) == 'Slices.SErviNGSliCes' \n",
    "    for Strongest_Extension('my_class', ['AA', 'Be', 'CC']) == 'my_class.AA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc15beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: HumanEval/153\n",
      "Prompt: \n",
      "def Strongest_Extension(class_name, extensions):\n",
      "    \"\"\"You will be given the name of a class (a string) and a list of extensions.\n",
      "    The extensions are to be used to load additional classes to the class. The\n",
      "    strength of the extension is as follows: Let CAP be the number of the uppercase\n",
      "    letters in the extension's name, and let SM be the number of lowercase letters \n",
      "    in the extension's name, the strength is given by the fraction CAP - SM. \n",
      "    You should find the strongest extension and return a string in this \n",
      "    format: ClassName.StrongestExtensionName.\n",
      "    If there are two or more extensions with the same strength, you should\n",
      "    choose the one that comes first in the list.\n",
      "    For example, if you are given \"Slices\" as the class and a list of the\n",
      "    extensions: ['SErviNGSliCes', 'Cheese', 'StuFfed'] then you should\n",
      "    return 'Slices.SErviNGSliCes' since 'SErviNGSliCes' is the strongest extension \n",
      "    (its strength is -1).\n",
      "    Example:\n",
      "    for Strongest_Extension('my_class', ['AA', 'Be', 'CC']) == 'my_class.AA'\n",
      "    \"\"\"\n",
      "\n",
      "Canonical Solution:     strong = extensions[0]\n",
      "    my_val = len([x for x in extensions[0] if x.isalpha() and x.isupper()]) - len([x for x in extensions[0] if x.isalpha() and x.islower()])\n",
      "    for s in extensions:\n",
      "        val = len([x for x in s if x.isalpha() and x.isupper()]) - len([x for x in s if x.isalpha() and x.islower()])\n",
      "        if val > my_val:\n",
      "            strong = s\n",
      "            my_val = val\n",
      "\n",
      "    ans = class_name + \".\" + strong\n",
      "    return ans\n",
      "\n",
      "\n",
      "Test Data: def check(candidate):\n",
      "\n",
      "    # Check some simple cases\n",
      "    assert candidate('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']) == 'Watashi.eIGHt8OKe'\n",
      "    assert candidate('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) == 'Boku123.YEs.WeCaNe'\n",
      "    assert candidate('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) == '__YESIMHERE.NuLl__'\n",
      "    assert candidate('K', ['Ta', 'TAR', 't234An', 'cosSo']) == 'K.TAR'\n",
      "    assert candidate('__HAHA', ['Tab', '123', '781345', '-_-']) == '__HAHA.123'\n",
      "    assert candidate('YameRore', ['HhAas', 'okIWILL123', 'WorkOut', 'Fails', '-_-']) == 'YameRore.okIWILL123'\n",
      "    assert candidate('finNNalLLly', ['Die', 'NowW', 'Wow', 'WoW']) == 'finNNalLLly.WoW'\n",
      "\n",
      "    # Check some edge cases that are easy to work out by hand.\n",
      "    assert candidate('_', ['Bb', '91245']) == '_.Bb'\n",
      "    assert candidate('Sp', ['671235', 'Bb']) == 'Sp.671235'\n",
      "    \n",
      "\n",
      "Entry Point: Strongest_Extension\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "# Initialize Task Index\n",
    "task_index = 153\n",
    "task_153 = dataset[\"test\"][task_index]\n",
    "\n",
    "# Access Task Details\n",
    "task_id = task_153[\"task_id\"]\n",
    "prompt = task_153[\"prompt\"]\n",
    "canonical_solution = task_153[\"canonical_solution\"]\n",
    "test_data = task_153[\"test\"]\n",
    "entry_point = task_153[\"entry_point\"]\n",
    "\n",
    "# Display Task Details\n",
    "print(f\"Task ID: {task_id}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Canonical Solution: {canonical_solution}\")\n",
    "print(f\"Test Data: {test_data}\")\n",
    "print(f\"Entry Point: {entry_point}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011630b",
   "metadata": {},
   "source": [
    "### Revised Prompt for Few-Shot\n",
    "\n",
    "write a Python code snippet that does not use a function or function definition that finds the strongest extension to load additional classes for a given class name. The strength of each extension is determined by the difference between the number of uppercase and lowercase letters in its name. If there are multiple extensions with the same strength, choose the one that appears first in the list. Return the class name concatenated with the strongest extension in the format \"ClassName.StrongestExtensionName\".\n",
    "\n",
    "Example:\n",
    "   Input: Class name: 'Slices', Extensions: ['SErviNGSliCes', 'Cheese', 'StuFfed']\n",
    "   Output: 'Slices.SErviNGSliCes'\n",
    "\n",
    "   Input: Class name: 'my_class', Extensions: ['AA', 'Be', 'CC']\n",
    "   Output: 'my_class.AA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383eda64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs:\n",
      "mod1_output1: Watashi.eIGHt8OKe\n",
      "mod1_output2: Boku123.YEs.WeCaNe\n",
      "mod1_output3: __YESIMHERE.NuLl__\n",
      "\n",
      "Stored execution times:\n",
      "execution_time_mod1_output1: 0.00000\n",
      "execution_time_mod1_output2: 0.00000\n",
      "execution_time_mod1_output3: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/20\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 1 \n",
    "\n",
    "import time\n",
    "\n",
    "# Define input pairs\n",
    "inputs = [\n",
    "    ('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']),\n",
    "    ('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']),\n",
    "    ('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321'])\n",
    "]\n",
    "\n",
    "# Initialize variables to store outputs\n",
    "mod1_output1 = None\n",
    "mod1_output2 = None\n",
    "mod1_output3 = None\n",
    "\n",
    "# Initialize variables to store execution times\n",
    "execution_time_mod1_output1 = None\n",
    "execution_time_mod1_output2 = None\n",
    "execution_time_mod1_output3 = None\n",
    "\n",
    "# Iterate over input pairs\n",
    "for index, (class_name, extensions) in enumerate(inputs):\n",
    "    strongest_extension = None\n",
    "    max_strength = float('-inf')\n",
    "\n",
    "    for ext in extensions:\n",
    "        strength = sum(1 for c in ext if c.isupper()) - sum(1 for c in ext if c.islower())\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if strength > max_strength:\n",
    "            max_strength = strength\n",
    "            strongest_extension = ext\n",
    "            \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "    # Store the output and execution time based on the index\n",
    "    if index == 0:\n",
    "        mod1_output1 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod1_output1 = execution_time\n",
    "    elif index == 1:\n",
    "        mod1_output2 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod1_output2 = execution_time\n",
    "    elif index == 2:\n",
    "        mod1_output3 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod1_output3 = execution_time\n",
    "\n",
    "# Print the stored outputs\n",
    "print(\"Stored outputs:\")\n",
    "print(\"mod1_output1:\", mod1_output1)\n",
    "print(\"mod1_output2:\", mod1_output2)\n",
    "print(\"mod1_output3:\", mod1_output3)\n",
    "\n",
    "print()\n",
    "# Print the stored execution times\n",
    "print(\"Stored execution times:\")\n",
    "print(\"execution_time_mod1_output1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "print(\"execution_time_mod1_output2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "print(\"execution_time_mod1_output3:\", format(execution_time_mod1_output3, '.5f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3b1bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.00000\n",
      "Execution time Model 1 Output 2: 0.00000\n",
      "Execution time Model 1 Output 3: 0.00000\n",
      "Average Execution Time for Model 1 Output: 0.00000\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 1 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/20\n",
    "# Function - Zero Shot\n",
    "# Overall Metrics\n",
    "\n",
    "## Model 1 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    if actual_output is None:\n",
    "        if expected_output is None:\n",
    "            return {\n",
    "                \"Precision\": 1,\n",
    "                \"Recall\": 1,\n",
    "                \"F1 Score\": 1,\n",
    "                \"Accuracy\": 1,\n",
    "                \"Exact Match\": 1\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"Precision\": 0,\n",
    "                \"Recall\": 0,\n",
    "                \"F1 Score\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Exact Match\": 0\n",
    "            }\n",
    "    \n",
    "    if isinstance(actual_output, list):\n",
    "        if None in actual_output:\n",
    "            actual_output = [x for x in actual_output if x is not None]\n",
    "\n",
    "        true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "        false_positives = max(len(actual_output) - true_positives, 0)\n",
    "        false_negatives = max(len(expected_output) - true_positives, 0)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "    else:\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "        return {\n",
    "            \"Precision\": exact_match_accuracy,\n",
    "            \"Recall\": exact_match_accuracy,\n",
    "            \"F1 Score\": exact_match_accuracy,\n",
    "            \"Accuracy\": exact_match_accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7cb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs:\n",
      "mod2_output1: Watashi.eIGHt8OKe\n",
      "mod2_output2: Boku123.YEs.WeCaNe\n",
      "mod2_output3: __YESIMHERE.NuLl__\n",
      "\n",
      "Stored execution times:\n",
      "execution_time_mod2_output1: 0.00000\n",
      "execution_time_mod2_output2: 0.00000\n",
      "execution_time_mod2_output3: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/20\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 2 \n",
    "\n",
    "import time\n",
    "\n",
    "# Define input pairs\n",
    "inputs = [\n",
    "    ('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']),\n",
    "    ('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']),\n",
    "    ('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321'])\n",
    "]\n",
    "\n",
    "# Initialize variables to store outputs and execution times\n",
    "mod2_output1 = None\n",
    "mod2_output2 = None\n",
    "mod2_output3 = None\n",
    "execution_time_mod2_output1 = None\n",
    "execution_time_mod2_output2 = None\n",
    "execution_time_mod2_output3 = None\n",
    "\n",
    "# Iterate over input pairs\n",
    "for index, (class_name, extensions) in enumerate(inputs):\n",
    "    strongest_extension = None\n",
    "    max_strength = float('-inf')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Find the strongest extension\n",
    "    for ext in extensions:\n",
    "        upper_count = sum(1 for c in ext if c.isupper())\n",
    "        lower_count = sum(1 for c in ext if c.islower())\n",
    "        strength = upper_count - lower_count\n",
    "        \n",
    "        if strength > max_strength:\n",
    "            max_strength = strength\n",
    "            strongest_extension = ext\n",
    "\n",
    "    # Store the output and execution time based on the index\n",
    "    if index == 0:\n",
    "        mod2_output1 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod2_output1 = time.time() - start_time\n",
    "    elif index == 1:\n",
    "        mod2_output2 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod2_output2 = time.time() - start_time\n",
    "    elif index == 2:\n",
    "        mod2_output3 = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_time_mod2_output3 = time.time() - start_time\n",
    "\n",
    "# Print the stored outputs\n",
    "print(\"Stored outputs:\")\n",
    "print(\"mod2_output1:\", mod2_output1)\n",
    "print(\"mod2_output2:\", mod2_output2)\n",
    "print(\"mod2_output3:\", mod2_output3)\n",
    "\n",
    "print()\n",
    "# Print the stored execution times\n",
    "print(\"Stored execution times:\")\n",
    "print(\"execution_time_mod2_output1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "print(\"execution_time_mod2_output2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "print(\"execution_time_mod2_output3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20cb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.00000\n",
      "Execution time Model 2 Output 2: 0.00000\n",
      "Execution time Model 2 Output 3: 0.00000\n",
      "Average Execution Time for Model 2 Output: 0.00000\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 2 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function - Zero Shot\n",
    "\n",
    "## Model 2 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(actual_output, list):\n",
    "        true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "        false_positives = max(len(actual_output) - true_positives, 0)\n",
    "        false_negatives = max(len(expected_output) - true_positives, 0)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "    else:\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "        return {\n",
    "            \"Precision\": exact_match_accuracy,\n",
    "            \"Recall\": exact_match_accuracy,\n",
    "            \"F1 Score\": exact_match_accuracy,\n",
    "            \"Accuracy\": exact_match_accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod2 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod2)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod2)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics2_new = pd.concat([df_metrics2_new, avg_metrics2.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eec047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs:\n",
      "mod3_output1: Watashi.eIGHt8OKe\n",
      "mod3_output2: Boku123.YEs.WeCaNe\n",
      "mod3_output3: __YESIMHERE.NuLl__\n",
      "\n",
      "Stored execution times:\n",
      "execution_time_mod3_output1: 0.00000\n",
      "execution_time_mod3_output2: 0.00000\n",
      "execution_time_mod3_output3: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 3\n",
    "\n",
    "import time\n",
    "\n",
    "# Define input pairs\n",
    "inputs = [\n",
    "    ('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']),\n",
    "    ('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']),\n",
    "    ('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321'])\n",
    "]\n",
    "\n",
    "# Initialize lists to store outputs and execution times\n",
    "mod3_outputs = [None] * len(inputs)\n",
    "execution_times_mod3 = [None] * len(inputs)\n",
    "\n",
    "# Initialize index variable\n",
    "index = 0\n",
    "\n",
    "# Iterate over input pairs\n",
    "while index < len(inputs):\n",
    "    # Unpack input pair\n",
    "    class_name, extensions = inputs[index]\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize variables to track the strongest extension\n",
    "    strongest_extension = None\n",
    "    max_strength = float('-inf')\n",
    "\n",
    "    # Initialize index for extensions\n",
    "    ext_index = 0\n",
    "\n",
    "    # Iterate over extensions to find the strongest one\n",
    "    while ext_index < len(extensions):\n",
    "        ext = extensions[ext_index]\n",
    "        upper_count = sum(1 for c in ext if c.isupper())\n",
    "        lower_count = sum(1 for c in ext if c.islower())\n",
    "        strength = upper_count - lower_count\n",
    "\n",
    "        if strength > max_strength:\n",
    "            max_strength = strength\n",
    "            strongest_extension = ext\n",
    "\n",
    "        # Move to the next extension\n",
    "        ext_index += 1\n",
    "\n",
    "    # Store the output and execution time based on the index\n",
    "    if index == 0:\n",
    "        mod3_outputs[0] = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_times_mod3[0] = time.time() - start_time\n",
    "    elif index == 1:\n",
    "        mod3_outputs[1] = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_times_mod3[1] = time.time() - start_time\n",
    "    elif index == 2:\n",
    "        mod3_outputs[2] = f\"{class_name}.{strongest_extension}\" if strongest_extension else None\n",
    "        execution_times_mod3[2] = time.time() - start_time\n",
    "\n",
    "    # Move to the next input pair\n",
    "    index += 1\n",
    "\n",
    "# Print the stored outputs\n",
    "print(\"Stored outputs:\")\n",
    "print(\"mod3_output1:\", mod3_outputs[0])\n",
    "print(\"mod3_output2:\", mod3_outputs[1])\n",
    "print(\"mod3_output3:\", mod3_outputs[2])\n",
    "\n",
    "print()\n",
    "# Print the stored execution times\n",
    "print(\"Stored execution times:\")\n",
    "print(\"execution_time_mod3_output1:\", format(execution_times_mod3[0], '.5f'))\n",
    "print(\"execution_time_mod3_output2:\", format(execution_times_mod3[1], '.5f'))\n",
    "print(\"execution_time_mod3_output3:\", format(execution_times_mod3[2], '.5f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8f45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.00000\n",
      "Execution time Model 3 Output 2: 0.00000\n",
      "Execution time Model 3 Output 3: 0.00000\n",
      "Average Execution Time for Model 3 Output: 0.00000\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 3 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function - Zero Shot\n",
    "# Model 3 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_times_mod3[0], '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_times_mod3[1], '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_times_mod3[2], '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_times_mod3[0] + execution_times_mod3[1] + execution_times_mod3[2]) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    if actual_output is None:\n",
    "        if expected_output is None:\n",
    "            return {\n",
    "                \"Precision\": 1,\n",
    "                \"Recall\": 1,\n",
    "                \"F1 Score\": 1,\n",
    "                \"Accuracy\": 1,\n",
    "                \"Exact Match\": 1\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"Precision\": 0,\n",
    "                \"Recall\": 0,\n",
    "                \"F1 Score\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Exact Match\": 0\n",
    "            }\n",
    "    \n",
    "    if isinstance(actual_output, list):\n",
    "        if None in actual_output:\n",
    "            actual_output = [x for x in actual_output if x is not None]\n",
    "\n",
    "        true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "        false_positives = max(len(actual_output) - true_positives, 0)\n",
    "        false_negatives = max(len(expected_output) - true_positives, 0)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "    else:\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "        return {\n",
    "            \"Precision\": exact_match_accuracy,\n",
    "            \"Recall\": exact_match_accuracy,\n",
    "            \"F1 Score\": exact_match_accuracy,\n",
    "            \"Accuracy\": exact_match_accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod3 = [mod3_outputs[0], mod3_outputs[1], mod3_outputs[2]]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod3)\n",
    "\n",
    "metrics3 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod3)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics3_new = pd.DataFrame(metrics3)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "avg_metrics3.name = 'Model 3 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics3_new = pd.concat([df_metrics3_new, avg_metrics3.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics3_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec7a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Precision  Recall  F1 Score  Accuracy  Exact Match  \\\n",
      "Model 1                     1.0     1.0       1.0       1.0          1.0   \n",
      "Model 2                     1.0     1.0       1.0       1.0          1.0   \n",
      "Model 3                     1.0     1.0       1.0       1.0          1.0   \n",
      "Overall Average             1.0     1.0       1.0       1.0          1.0   \n",
      "Overall Average Time        0.0     0.0       0.0       0.0          0.0   \n",
      "\n",
      "                      Average Time  \n",
      "Model 1                        0.0  \n",
      "Model 2                        0.0  \n",
      "Model 3                        0.0  \n",
      "Overall Average                0.0  \n",
      "Overall Average Time           0.0  \n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Overall Metrics\n",
    "\n",
    "# Calculate the averages for each set of metrics\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "\n",
    "# Calculate the overall average time\n",
    "overall_average_time = (average_time_mod1output + average_time_mod2output + average_time_mod3output) / 3\n",
    "\n",
    "# Create a DataFrame to store the averages\n",
    "df_avg_metrics = pd.DataFrame([avg_metrics1, avg_metrics2, avg_metrics3])\n",
    "\n",
    "# Add a new column for average time\n",
    "df_avg_metrics['Average Time'] = [average_time_mod1output, average_time_mod2output, average_time_mod3output]\n",
    "\n",
    "# Add a new row displaying overall average\n",
    "df_avg_metrics.loc['Overall Average'] = df_avg_metrics.mean()\n",
    "\n",
    "# Add a new row for overall average time\n",
    "df_avg_metrics.loc['Overall Average Time'] = overall_average_time\n",
    "\n",
    "# Rename the index to include model names\n",
    "df_avg_metrics.index = ['Model 1', 'Model 2', 'Model 3', 'Overall Average', 'Overall Average Time']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ec726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bfc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
