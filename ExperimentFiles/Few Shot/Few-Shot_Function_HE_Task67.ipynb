{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71c150c",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: Pauline Armamento - U3246782 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Few-shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752494d",
   "metadata": {},
   "source": [
    "### HumanEval\n",
    "\n",
    "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models.\n",
    "https://huggingface.co/datasets/openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df98697",
   "metadata": {},
   "source": [
    "# Task ID 67\n",
    "Prompt: Write a Python function that takes a string representing the total number of oranges and apples, and an integer representing the total number of fruits, and returns the number of mango fruits in the basket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3539a",
   "metadata": {},
   "source": [
    "### Revised Prompt for Few Shot\n",
    "Prompt: You're provided with a string indicating the number of apples and oranges in a fruit basket, along with an integer representing the total number of fruits in the basket. write a Python code snippet that does not use a function or function definition that finds the number of mango fruits in the basket. To do so, subtract the number of apples and oranges from the total number of fruits. Return this count as the number of mango fruits.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: String: \"\"5 apples and 6 oranges\"\", Total fruits: 19\n",
    "Output: 19 - 5 - 6 = 8\n",
    "\n",
    "Input: String: \"\"0 apples and 1 oranges\"\", Total fruits: 3\n",
    "Output: 3 - 0 - 1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc15beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: HumanEval/67\n",
      "Prompt: \n",
      "def fruit_distribution(s,n):\n",
      "    \"\"\"\n",
      "    In this task, you will be given a string that represents a number of apples and oranges \n",
      "    that are distributed in a basket of fruit this basket contains \n",
      "    apples, oranges, and mango fruits. Given the string that represents the total number of \n",
      "    the oranges and apples and an integer that represent the total number of the fruits \n",
      "    in the basket return the number of the mango fruits in the basket.\n",
      "    for examble:\n",
      "    fruit_distribution(\"5 apples and 6 oranges\", 19) ->19 - 5 - 6 = 8\n",
      "    fruit_distribution(\"0 apples and 1 oranges\",3) -> 3 - 0 - 1 = 2\n",
      "    fruit_distribution(\"2 apples and 3 oranges\", 100) -> 100 - 2 - 3 = 95\n",
      "    fruit_distribution(\"100 apples and 1 oranges\",120) -> 120 - 100 - 1 = 19\n",
      "    \"\"\"\n",
      "\n",
      "Canonical Solution:     lis = list()\n",
      "    for i in s.split(' '):\n",
      "        if i.isdigit():\n",
      "            lis.append(int(i))\n",
      "    return n - sum(lis)\n",
      "\n",
      "Test Data: def check(candidate):\n",
      "\n",
      "    # Check some simple cases\n",
      "    assert candidate(\"5 apples and 6 oranges\",19) == 8\n",
      "    assert candidate(\"5 apples and 6 oranges\",21) == 10\n",
      "    assert candidate(\"0 apples and 1 oranges\",3) == 2\n",
      "    assert candidate(\"1 apples and 0 oranges\",3) == 2\n",
      "    assert candidate(\"2 apples and 3 oranges\",100) == 95\n",
      "    assert candidate(\"2 apples and 3 oranges\",5) == 0\n",
      "    assert candidate(\"1 apples and 100 oranges\",120) == 19\n",
      "\n",
      "Entry Point: fruit_distribution\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "# Initialize Task Index\n",
    "task_index = 67\n",
    "task_153 = dataset[\"test\"][task_index]\n",
    "\n",
    "# Access Task Details\n",
    "task_id = task_153[\"task_id\"]\n",
    "prompt = task_153[\"prompt\"]\n",
    "canonical_solution = task_153[\"canonical_solution\"]\n",
    "test_data = task_153[\"test\"]\n",
    "entry_point = task_153[\"entry_point\"]\n",
    "\n",
    "# Display Task Details\n",
    "print(f\"Task ID: {task_id}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Canonical Solution: {canonical_solution}\")\n",
    "print(f\"Test Data: {test_data}\")\n",
    "print(f\"Entry Point: {entry_point}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7cb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.01171\n",
      "Execution time Model 1 Output 2: 0.01414\n",
      "Execution time Model 1 Output 3: 0.01224\n",
      "Average Execution Time for Model 1 Output: 0.01270\n",
      "Expected Output:  [8, 10, 2]\n",
      "Actual Output:  [8, 10, 2]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 1 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/67\n",
    "# Function Generation - Few Shot\n",
    "# Model 1\n",
    "\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "def fruit_distribution1(s, n):\n",
    "    # Split the string to extract the numbers of apples and oranges\n",
    "    parts = s.split()\n",
    "    apples = 0\n",
    "    oranges = 0\n",
    "    for i in range(len(parts)):\n",
    "        if parts[i].isdigit():\n",
    "            if parts[i + 1] == 'apples':\n",
    "                apples = int(parts[i])\n",
    "            elif parts[i + 1] == 'oranges':\n",
    "                oranges = int(parts[i])\n",
    "    \n",
    "    # Calculate the number of mangoes\n",
    "    mangoes = n - apples - oranges\n",
    "    \n",
    "    return mangoes\n",
    "\n",
    "\n",
    "# Test cases\n",
    "mod1_output1 = fruit_distribution1(\"5 apples and 6 oranges\",19) # 8\n",
    "mod1_output2 = fruit_distribution1(\"5 apples and 6 oranges\",21) # 10\n",
    "mod1_output3 = fruit_distribution1(\"1 apples and 0 oranges\",3) # 2\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod1_output1 = timeit.timeit(lambda: fruit_distribution1(\"5 apples and 6 oranges\",19), number=10000)\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "execution_time_mod1_output2 = timeit.timeit(lambda: fruit_distribution1(\"5 apples and 6 oranges\",21), number=10000)\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "\n",
    "execution_time_mod1_output3 = timeit.timeit(lambda: fruit_distribution1(\"1 apples and 0 oranges\",3), number=10000)\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [8, 10, 2]\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b20cb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.01452\n",
      "Execution time Model 2 Output 2: 0.01352\n",
      "Execution time Model 2 Output 3: 0.01267\n",
      "Average Execution Time for Model 2 Output: 0.01357\n",
      "Expected Output:  [8, 10, 2]\n",
      "Actual Output:  [8, 10, 2]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 2 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/95\n",
    "# Function Generation - Few Shot\n",
    "# Model 2\n",
    "\n",
    "\n",
    "def fruit_distribution2(s, n):\n",
    "    # Dictionary to store the counts of each fruit\n",
    "    fruit_counts = {'apples': 0, 'oranges': 0, 'mangoes': 0}\n",
    "\n",
    "    # Split the string and update the counts\n",
    "    parts = s.split()\n",
    "    for i in range(len(parts)):\n",
    "        if parts[i].isdigit():\n",
    "            fruit = parts[i + 1]\n",
    "            fruit_counts[fruit] = int(parts[i])\n",
    "\n",
    "    # Calculate the number of mangoes\n",
    "    mangoes = n - sum(fruit_counts.values())\n",
    "\n",
    "    return mangoes\n",
    "\n",
    "# Test cases\n",
    "mod2_output1 = fruit_distribution2(\"5 apples and 6 oranges\",19) # 8\n",
    "mod2_output2 = fruit_distribution2(\"5 apples and 6 oranges\",21) # 10\n",
    "mod2_output3 = fruit_distribution2(\"1 apples and 0 oranges\",3) # 2\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod2_output1 = timeit.timeit(lambda: fruit_distribution2(\"5 apples and 6 oranges\",19), number=10000)\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "execution_time_mod2_output2 = timeit.timeit(lambda: fruit_distribution2(\"5 apples and 6 oranges\",21), number=10000)\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "execution_time_mod2_output3 = timeit.timeit(lambda: fruit_distribution2(\"1 apples and 0 oranges\",3), number=10000)\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [8, 10, 2]\n",
    "actual_outputs_mod2 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod2)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod2)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics2_new = pd.concat([df_metrics2_new, avg_metrics2.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec8f45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.01806\n",
      "Execution time Model 3 Output 2: 0.01458\n",
      "Execution time Model 3 Output 3: 0.01452\n",
      "Average Execution Time for Model 3 Output: 0.01572\n",
      "Expected Output:  [8, 10, 2]\n",
      "Actual Output:  [8, 10, 2]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 3 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/67\n",
    "# Function Generation - Few Shot\n",
    "# Model 3\n",
    "\n",
    "import re\n",
    "\n",
    "def fruit_distribution3(s, n):\n",
    "    # Use regular expression to find the counts of apples and oranges\n",
    "    matches = re.findall(r'(\\d+) (apples|oranges)', s)\n",
    "    apples = 0\n",
    "    oranges = 0\n",
    "    for count, fruit in matches:\n",
    "        if fruit == 'apples':\n",
    "            apples += int(count)\n",
    "        elif fruit == 'oranges':\n",
    "            oranges += int(count)\n",
    "\n",
    "    # Calculate the number of mangoes\n",
    "    mangoes = n - apples - oranges\n",
    "\n",
    "    return mangoes\n",
    "\n",
    "# Test cases\n",
    "mod3_output1 = fruit_distribution3(\"5 apples and 6 oranges\",19) # 8\n",
    "mod3_output2 = fruit_distribution3(\"5 apples and 6 oranges\",21) # 10\n",
    "mod3_output3 = fruit_distribution3(\"1 apples and 0 oranges\",3) # 2\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod3_output1 = timeit.timeit(lambda: fruit_distribution3(\"5 apples and 6 oranges\",19), number=10000)\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "execution_time_mod3_output2 = timeit.timeit(lambda: fruit_distribution3(\"5 apples and 6 oranges\",21), number=10000)\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "execution_time_mod3_output3 = timeit.timeit(lambda: fruit_distribution3(\"1 apples and 0 oranges\",3), number=10000)\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output3, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [8, 10, 2]\n",
    "actual_outputs_mod3 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod3)\n",
    "\n",
    "metrics3 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod3)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics3_new = pd.DataFrame(metrics3)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "avg_metrics3.name = 'Model 3 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics3_new = pd.concat([df_metrics3_new, avg_metrics3.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics3_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec7a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Precision    Recall  F1 Score  Accuracy  Exact Match  \\\n",
      "Model 1                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Model 2                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Model 3                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Overall Average        1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Overall Average Time   0.013996  0.013996  0.013996  0.013996     0.013996   \n",
      "\n",
      "                      Average Time  \n",
      "Model 1                   0.012698  \n",
      "Model 2                   0.013569  \n",
      "Model 3                   0.015722  \n",
      "Overall Average           0.013996  \n",
      "Overall Average Time      0.013996  \n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/67\n",
    "# Overall Metrics\n",
    "\n",
    "# Calculate the averages for each set of metrics\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "\n",
    "# Calculate the overall average time\n",
    "overall_average_time = (average_time_mod1output + average_time_mod2output + average_time_mod3output) / 3\n",
    "\n",
    "# Create a DataFrame to store the averages\n",
    "df_avg_metrics = pd.DataFrame([avg_metrics1, avg_metrics2, avg_metrics3])\n",
    "\n",
    "# Add a new column for average time\n",
    "df_avg_metrics['Average Time'] = [average_time_mod1output, average_time_mod2output, average_time_mod3output]\n",
    "\n",
    "# Add a new row displaying overall average\n",
    "df_avg_metrics.loc['Overall Average'] = df_avg_metrics.mean()\n",
    "\n",
    "# Add a new row for overall average time\n",
    "df_avg_metrics.loc['Overall Average Time'] = overall_average_time\n",
    "\n",
    "# Rename the index to include model names\n",
    "df_avg_metrics.index = ['Model 1', 'Model 2', 'Model 3', 'Overall Average', 'Overall Average Time']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ec726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bfc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
