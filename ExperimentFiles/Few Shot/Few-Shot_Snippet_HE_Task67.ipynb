{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71c150c",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: Pauline Armamento - U3246782 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Few-shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752494d",
   "metadata": {},
   "source": [
    "### HumanEval\n",
    "\n",
    "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models.\n",
    "https://huggingface.co/datasets/openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef3484f",
   "metadata": {},
   "source": [
    "# Task ID 67\n",
    "Prompt: Write a Python function that takes a string representing the total number of oranges and apples, and an integer representing the total number of fruits, and returns the number of mango fruits in the basket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fea4c",
   "metadata": {},
   "source": [
    "### Revised Prompt for Few Shot\n",
    "Prompt: In this task, you will be given a string that represents a number of apples and oranges \n",
    "    that are distributed in a basket of fruit this basket contains \n",
    "    apples, oranges, and mango fruits. Given the string that represents the total number of \n",
    "    the oranges and apples and an integer that represent the total number of the fruits \n",
    "    in the basket return the number of the mango fruits in the basket.\n",
    "\n",
    "    Examble:\n",
    "    fruit_distribution(\"\"5 apples and 6 oranges\"\", 19) ->19 - 5 - 6 = 8\n",
    "    fruit_distribution(\"\"0 apples and 1 oranges\"\",3) -> 3 - 0 - 1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc15beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: HumanEval/67\n",
      "Prompt: \n",
      "def fruit_distribution(s,n):\n",
      "    \"\"\"\n",
      "    In this task, you will be given a string that represents a number of apples and oranges \n",
      "    that are distributed in a basket of fruit this basket contains \n",
      "    apples, oranges, and mango fruits. Given the string that represents the total number of \n",
      "    the oranges and apples and an integer that represent the total number of the fruits \n",
      "    in the basket return the number of the mango fruits in the basket.\n",
      "    for examble:\n",
      "    fruit_distribution(\"5 apples and 6 oranges\", 19) ->19 - 5 - 6 = 8\n",
      "    fruit_distribution(\"0 apples and 1 oranges\",3) -> 3 - 0 - 1 = 2\n",
      "    fruit_distribution(\"2 apples and 3 oranges\", 100) -> 100 - 2 - 3 = 95\n",
      "    fruit_distribution(\"100 apples and 1 oranges\",120) -> 120 - 100 - 1 = 19\n",
      "    \"\"\"\n",
      "\n",
      "Canonical Solution:     lis = list()\n",
      "    for i in s.split(' '):\n",
      "        if i.isdigit():\n",
      "            lis.append(int(i))\n",
      "    return n - sum(lis)\n",
      "\n",
      "Test Data: def check(candidate):\n",
      "\n",
      "    # Check some simple cases\n",
      "    assert candidate(\"5 apples and 6 oranges\",19) == 8\n",
      "    assert candidate(\"5 apples and 6 oranges\",21) == 10\n",
      "    assert candidate(\"0 apples and 1 oranges\",3) == 2\n",
      "    assert candidate(\"1 apples and 0 oranges\",3) == 2\n",
      "    assert candidate(\"2 apples and 3 oranges\",100) == 95\n",
      "    assert candidate(\"2 apples and 3 oranges\",5) == 0\n",
      "    assert candidate(\"1 apples and 100 oranges\",120) == 19\n",
      "\n",
      "Entry Point: fruit_distribution\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "# Initialize Task Index\n",
    "task_index = 67\n",
    "task_153 = dataset[\"test\"][task_index]\n",
    "\n",
    "# Access Task Details\n",
    "task_id = task_153[\"task_id\"]\n",
    "prompt = task_153[\"prompt\"]\n",
    "canonical_solution = task_153[\"canonical_solution\"]\n",
    "test_data = task_153[\"test\"]\n",
    "entry_point = task_153[\"entry_point\"]\n",
    "\n",
    "# Display Task Details\n",
    "print(f\"Task ID: {task_id}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Canonical Solution: {canonical_solution}\")\n",
    "print(f\"Test Data: {test_data}\")\n",
    "print(f\"Entry Point: {entry_point}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011630b",
   "metadata": {},
   "source": [
    "### Revised Prompt for Few-Shot\n",
    "You're provided with a string indicating the number of apples and oranges in a fruit basket, along with an integer representing the total number of fruits in the basket. write a Python code snippet that does not use a function or function definition that finds the number of mango fruits in the basket. To do so, subtract the number of apples and oranges from the total number of fruits. Return this count as the number of mango fruits.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: String: \"5 apples and 6 oranges\", Total fruits: 19\n",
    "Output: 19 - 5 - 6 = 8\n",
    "\n",
    "Input: String: \"0 apples and 1 oranges\", Total fruits: 3\n",
    "Output: 3 - 0 - 1 = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383eda64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs:\n",
      "mod1_output1: 8\n",
      "mod1_output2: 10\n",
      "mod1_output3: 2\n",
      "\n",
      "Stored execution times:\n",
      "execution_time_mod1_output1: 0.00000\n",
      "execution_time_mod1_output2: 0.00000\n",
      "execution_time_mod1_output3: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/107\n",
    "# Code Snippet - Few Shot\n",
    "# Model 1 \n",
    "\n",
    "import time\n",
    "\n",
    "# Define input pairs\n",
    "inputs = [\n",
    "    (\"5 apples and 6 oranges\", 19),\n",
    "    (\"5 apples and 6 oranges\", 21),\n",
    "    (\"1 apples and 0 oranges\", 3)\n",
    "]\n",
    "\n",
    "# Initialize variables to store outputs\n",
    "mod1_output1 = None\n",
    "mod1_output2 = None\n",
    "mod1_output3 = None\n",
    "\n",
    "# Initialize variables to store execution times\n",
    "execution_time_mod1_output1 = None\n",
    "execution_time_mod1_output2 = None\n",
    "execution_time_mod1_output3 = None\n",
    "\n",
    "# Iterate over input pairs\n",
    "for index, (fruit_string, total_fruits) in enumerate(inputs):\n",
    "    # Split the string to extract the number of apples and oranges\n",
    "    fruit_counts = fruit_string.split()\n",
    "    apples = int(fruit_counts[0])\n",
    "    oranges = int(fruit_counts[3])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate the number of mangoes\n",
    "    mangoes = total_fruits - apples - oranges\n",
    "                \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Store the output and execution time based on the index\n",
    "    if index == 0:\n",
    "        mod1_output1 = mangoes\n",
    "        execution_time_mod1_output1 = execution_time\n",
    "    elif index == 1:\n",
    "        mod1_output2 = mangoes\n",
    "        execution_time_mod1_output2 = execution_time\n",
    "    elif index == 2:\n",
    "        mod1_output3 = mangoes\n",
    "        execution_time_mod1_output3 = execution_time\n",
    "\n",
    "# Print the stored outputs\n",
    "print(\"Stored outputs:\")\n",
    "print(\"mod1_output1:\", mod1_output1)\n",
    "print(\"mod1_output2:\", mod1_output2)\n",
    "print(\"mod1_output3:\", mod1_output3)\n",
    "\n",
    "print()\n",
    "# Print the stored execution times\n",
    "print(\"Stored execution times:\")\n",
    "print(\"execution_time_mod1_output1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "print(\"execution_time_mod1_output2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "print(\"execution_time_mod1_output3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3b1bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.00000\n",
      "Execution time Model 1 Output 2: 0.00000\n",
      "Execution time Model 1 Output 3: 0.00000\n",
      "Average Execution Time for Model 1 Output: 0.00000\n",
      "Expected Output:  [8, 10, 2]\n",
      "Actual Output:  [8, 10, 2]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 1 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/107\n",
    "# Function - Few Shot\n",
    "# Overall Metrics\n",
    "\n",
    "## Model 1 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    if actual_output is None:\n",
    "        if expected_output is None:\n",
    "            return {\n",
    "                \"Precision\": 1,\n",
    "                \"Recall\": 1,\n",
    "                \"F1 Score\": 1,\n",
    "                \"Accuracy\": 1,\n",
    "                \"Exact Match\": 1\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"Precision\": 0,\n",
    "                \"Recall\": 0,\n",
    "                \"F1 Score\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Exact Match\": 0\n",
    "            }\n",
    "    \n",
    "    if isinstance(actual_output, list):\n",
    "        if None in actual_output:\n",
    "            actual_output = [x for x in actual_output if x is not None]\n",
    "\n",
    "        true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "        false_positives = max(len(actual_output) - true_positives, 0)\n",
    "        false_negatives = max(len(expected_output) - true_positives, 0)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "    else:\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "        return {\n",
    "            \"Precision\": exact_match_accuracy,\n",
    "            \"Recall\": exact_match_accuracy,\n",
    "            \"F1 Score\": exact_match_accuracy,\n",
    "            \"Accuracy\": exact_match_accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "\n",
    "\n",
    "expected_outputs = [8, 10, 2]\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7cb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs for Mod2:\n",
      "mod2_output1: 8\n",
      "mod2_output2: 10\n",
      "mod2_output3: 2\n",
      "\n",
      "Stored execution times for Mod2:\n",
      "execution_time_mod2_output1: 0.00000\n",
      "execution_time_mod2_output2: 0.00000\n",
      "execution_time_mod2_output3: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/107\n",
    "# Code Snippet - Few Shot\n",
    "# Model 2 \n",
    "\n",
    "import time\n",
    "\n",
    "# Define input pairs\n",
    "inputs = [\n",
    "    (\"5 apples and 6 oranges\", 19),\n",
    "    (\"5 apples and 6 oranges\", 21),\n",
    "    (\"1 apples and 0 oranges\", 3)\n",
    "]\n",
    "\n",
    "# Initialize variables to store outputs and execution times\n",
    "mod2_output1 = None\n",
    "mod2_output2 = None\n",
    "mod2_output3 = None\n",
    "\n",
    "execution_time_mod2_output1 = None\n",
    "execution_time_mod2_output2 = None\n",
    "execution_time_mod2_output3 = None\n",
    "\n",
    "# Iterate over input pairs\n",
    "for index, (fruit_string, total_fruits) in enumerate(inputs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Split the string to extract the individual fruit types\n",
    "    fruits = fruit_string.split()\n",
    "    \n",
    "    # Initialize variable to count the number of mangoes\n",
    "    mangoes = 0\n",
    "    \n",
    "    # Iterate over each word in the fruit string\n",
    "    for word in fruits:\n",
    "        # Check if the word is a number\n",
    "        if word.isdigit():\n",
    "            # Convert the word to an integer and add it to the count of mangoes\n",
    "            mangoes += int(word)\n",
    "    \n",
    "    # Calculate the number of mangoes by subtracting the total fruits from the count of other fruits\n",
    "    mangoes = total_fruits - mangoes\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Store the output and execution time based on the index\n",
    "    if index == 0:\n",
    "        mod2_output1 = mangoes\n",
    "        execution_time_mod2_output1 = execution_time\n",
    "    elif index == 1:\n",
    "        mod2_output2 = mangoes\n",
    "        execution_time_mod2_output2 = execution_time\n",
    "    elif index == 2:\n",
    "        mod2_output3 = mangoes\n",
    "        execution_time_mod2_output3 = execution_time\n",
    "\n",
    "# Print the stored outputs\n",
    "print(\"Stored outputs for Mod2:\")\n",
    "print(\"mod2_output1:\", mod2_output1)\n",
    "print(\"mod2_output2:\", mod2_output2)\n",
    "print(\"mod2_output3:\", mod2_output3)\n",
    "\n",
    "print()\n",
    "# Print the stored execution times\n",
    "print(\"Stored execution times for Mod2:\")\n",
    "print(\"execution_time_mod2_output1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "print(\"execution_time_mod2_output2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "print(\"execution_time_mod2_output3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20cb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.00000\n",
      "Execution time Model 2 Output 2: 0.00000\n",
      "Execution time Model 2 Output 3: 0.00000\n",
      "Average Execution Time for Model 2 Output: 0.00000\n",
      "Expected Output:  [8, 10, 2]\n",
      "Actual Output:  [8, 10, 2]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 2 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/107\n",
    "# Function - Few Shot\n",
    "\n",
    "## Model 2 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(actual_output, list):\n",
    "        true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "        false_positives = max(len(actual_output) - true_positives, 0)\n",
    "        false_negatives = max(len(expected_output) - true_positives, 0)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "    else:\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "        return {\n",
    "            \"Precision\": exact_match_accuracy,\n",
    "            \"Recall\": exact_match_accuracy,\n",
    "            \"F1 Score\": exact_match_accuracy,\n",
    "            \"Accuracy\": exact_match_accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "expected_outputs = [8, 10, 2]\n",
    "actual_outputs_mod2 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod2)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod2)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics2_new = pd.concat([df_metrics2_new, avg_metrics2.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eec047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs for Mod3:\n",
      "mod3_output1: 19\n",
      "mod3_output2: 21\n",
      "mod3_output3: 3\n",
      "\n",
      "Stored execution times for Mod3:\n",
      "execution_time_mod3_output1: 0.00000\n",
      "execution_time_mod3_output2: 0.00000\n",
      "execution_time_mod3_output3: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/107\n",
    "# Code Snippet - Few Shot\n",
    "# Model 3\n",
    "\n",
    "import time\n",
    "\n",
    "# Define input pairs\n",
    "inputs = [\n",
    "    (\"5 apples and 6 oranges\", 19),\n",
    "    (\"5 apples and 6 oranges\", 21),\n",
    "    (\"1 apples and 0 oranges\", 3)\n",
    "]\n",
    "\n",
    "# Initialize variables to store outputs and execution times\n",
    "mod3_output1 = None\n",
    "mod3_output2 = None\n",
    "mod3_output3 = None\n",
    "\n",
    "execution_time_mod3_output1 = None\n",
    "execution_time_mod3_output2 = None\n",
    "execution_time_mod3_output3 = None\n",
    "\n",
    "# Iterate over input pairs\n",
    "for index, (fruit_string, total_fruits) in enumerate(inputs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Count the number of occurrences of the word \"mango\" in the fruit string\n",
    "    mango_count = fruit_string.lower().count(\"mango\")\n",
    "    \n",
    "    # Calculate the number of mangoes by subtracting the total fruits from the count of other fruits\n",
    "    mangoes = total_fruits - mango_count\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Store the output and execution time based on the index\n",
    "    if index == 0:\n",
    "        mod3_output1 = mangoes\n",
    "        execution_time_mod3_output1 = execution_time\n",
    "    elif index == 1:\n",
    "        mod3_output2 = mangoes\n",
    "        execution_time_mod3_output2 = execution_time\n",
    "    elif index == 2:\n",
    "        mod3_output3 = mangoes\n",
    "        execution_time_mod3_output3 = execution_time\n",
    "\n",
    "# Print the stored outputs\n",
    "print(\"Stored outputs for Mod3:\")\n",
    "print(\"mod3_output1:\", mod3_output1)\n",
    "print(\"mod3_output2:\", mod3_output2)\n",
    "print(\"mod3_output3:\", mod3_output3)\n",
    "\n",
    "print()\n",
    "# Print the stored execution times\n",
    "print(\"Stored execution times for Mod3:\")\n",
    "print(\"execution_time_mod3_output1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "print(\"execution_time_mod3_output2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "print(\"execution_time_mod3_output3:\", format(execution_time_mod3_output3, '.5f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8f45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.00000\n",
      "Execution time Model 3 Output 2: 0.00000\n",
      "Execution time Model 3 Output 3: 0.00000\n",
      "Average Execution Time for Model 3 Output: 0.00000\n",
      "Expected Output:  [8, 10, 2]\n",
      "Actual Output:  [19, 21, 3]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      0.0     0.0       0.0       0.0          0.0\n",
      "1                      0.0     0.0       0.0       0.0          0.0\n",
      "2                      0.0     0.0       0.0       0.0          0.0\n",
      "Model 3 Average        0.0     0.0       0.0       0.0          0.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/107\n",
    "# Function - Few Shot\n",
    "# Model 3 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "    if actual_output is None:\n",
    "        if expected_output is None:\n",
    "            return {\n",
    "                \"Precision\": 1,\n",
    "                \"Recall\": 1,\n",
    "                \"F1 Score\": 1,\n",
    "                \"Accuracy\": 1,\n",
    "                \"Exact Match\": 1\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"Precision\": 0,\n",
    "                \"Recall\": 0,\n",
    "                \"F1 Score\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Exact Match\": 0\n",
    "            }\n",
    "    \n",
    "    if isinstance(actual_output, list):\n",
    "        if None in actual_output:\n",
    "            actual_output = [x for x in actual_output if x is not None]\n",
    "\n",
    "        true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "        false_positives = max(len(actual_output) - true_positives, 0)\n",
    "        false_negatives = max(len(expected_output) - true_positives, 0)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "    else:\n",
    "        exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "        return {\n",
    "            \"Precision\": exact_match_accuracy,\n",
    "            \"Recall\": exact_match_accuracy,\n",
    "            \"F1 Score\": exact_match_accuracy,\n",
    "            \"Accuracy\": exact_match_accuracy,\n",
    "            \"Exact Match\": exact_match_accuracy\n",
    "        }\n",
    "\n",
    "\n",
    "expected_outputs = [8, 10, 2]\n",
    "actual_outputs_mod3 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod3)\n",
    "\n",
    "metrics3 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod3)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics3_new = pd.DataFrame(metrics3)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "avg_metrics3.name = 'Model 3 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics3_new = pd.concat([df_metrics3_new, avg_metrics3.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics3_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec7a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Precision    Recall  F1 Score  Accuracy  Exact Match  \\\n",
      "Model 1                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Model 2                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Model 3                0.000000  0.000000  0.000000  0.000000     0.000000   \n",
      "Overall Average        0.666667  0.666667  0.666667  0.666667     0.666667   \n",
      "Overall Average Time   0.000000  0.000000  0.000000  0.000000     0.000000   \n",
      "\n",
      "                      Average Time  \n",
      "Model 1                        0.0  \n",
      "Model 2                        0.0  \n",
      "Model 3                        0.0  \n",
      "Overall Average                0.0  \n",
      "Overall Average Time           0.0  \n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/107\n",
    "# Overall Metrics\n",
    "\n",
    "# Calculate the averages for each set of metrics\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "\n",
    "# Calculate the overall average time\n",
    "overall_average_time = (average_time_mod1output + average_time_mod2output + average_time_mod3output) / 3\n",
    "\n",
    "# Create a DataFrame to store the averages\n",
    "df_avg_metrics = pd.DataFrame([avg_metrics1, avg_metrics2, avg_metrics3])\n",
    "\n",
    "# Add a new column for average time\n",
    "df_avg_metrics['Average Time'] = [average_time_mod1output, average_time_mod2output, average_time_mod3output]\n",
    "\n",
    "# Add a new row displaying overall average\n",
    "df_avg_metrics.loc['Overall Average'] = df_avg_metrics.mean()\n",
    "\n",
    "# Add a new row for overall average time\n",
    "df_avg_metrics.loc['Overall Average Time'] = overall_average_time\n",
    "\n",
    "# Rename the index to include model names\n",
    "df_avg_metrics.index = ['Model 1', 'Model 2', 'Model 3', 'Overall Average', 'Overall Average Time']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ec726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bfc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
