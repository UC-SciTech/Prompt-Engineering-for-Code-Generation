{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71c150c",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: Pauline Armamento - U3246782 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Few-shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752494d",
   "metadata": {},
   "source": [
    "### HumanEval\n",
    "\n",
    "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models.\n",
    "https://huggingface.co/datasets/openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184e33c",
   "metadata": {},
   "source": [
    "# Task ID 95\n",
    "Prompt: Write a Python function where the input of dictionary returns True if all keys are strings in lower case or upper case, else False. Handle empty dictionaries as False. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1435e184",
   "metadata": {},
   "source": [
    "### Revised Prompt for Chain Of Thought\n",
    "Prompt: We want to determine if a dictionary has consistent key casing (all lowercase or all uppercase). Let's think about this step-by-step:\n",
    "\n",
    "1. Define Consistency: What constitutes consistent casing? We can define consistency as either all keys being lowercase or all keys being uppercase.\n",
    "\n",
    "2. Check Keys: How can we check the casing of each key? We can iterate through the dictionary and examine each key using built-in string methods like islower() and isupper().\n",
    "\n",
    "3. Track Consistency: As we iterate, we need to track if the casing is consistent. We can initialize two variables: all_lower and all_upper. We set all_lower to True if the first key is lowercase and update it to False if we encounter an uppercase key. Similarly, we set all_upper to True for the first uppercase key and update it to False for lowercase keys.\n",
    "\n",
    "4. Empty Dictionary:  We should also handle the case of an empty dictionary. An empty dictionary shouldn't be considered consistent casing, so we can return False directly for an empty input.\n",
    "\n",
    "5. Return Result:  After iterating through all keys, if either all_lower or all_upper remains True, the dictionary has consistent casing. Otherwise, it doesn't. We can return the final result based on these flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc15beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: HumanEval/95\n",
      "Prompt: \n",
      "def check_dict_case(dict):\n",
      "    \"\"\"\n",
      "    Given a dictionary, return True if all keys are strings in lower \n",
      "    case or all keys are strings in upper case, else return False.\n",
      "    The function should return False is the given dictionary is empty.\n",
      "    Examples:\n",
      "    check_dict_case({\"a\":\"apple\", \"b\":\"banana\"}) should return True.\n",
      "    check_dict_case({\"a\":\"apple\", \"A\":\"banana\", \"B\":\"banana\"}) should return False.\n",
      "    check_dict_case({\"a\":\"apple\", 8:\"banana\", \"a\":\"apple\"}) should return False.\n",
      "    check_dict_case({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"}) should return False.\n",
      "    check_dict_case({\"STATE\":\"NC\", \"ZIP\":\"12345\" }) should return True.\n",
      "    \"\"\"\n",
      "\n",
      "Canonical Solution:     if len(dict.keys()) == 0:\n",
      "        return False\n",
      "    else:\n",
      "        state = \"start\"\n",
      "        for key in dict.keys():\n",
      "\n",
      "            if isinstance(key, str) == False:\n",
      "                state = \"mixed\"\n",
      "                break\n",
      "            if state == \"start\":\n",
      "                if key.isupper():\n",
      "                    state = \"upper\"\n",
      "                elif key.islower():\n",
      "                    state = \"lower\"\n",
      "                else:\n",
      "                    break\n",
      "            elif (state == \"upper\" and not key.isupper()) or (state == \"lower\" and not key.islower()):\n",
      "                    state = \"mixed\"\n",
      "                    break\n",
      "            else:\n",
      "                break\n",
      "        return state == \"upper\" or state == \"lower\" \n",
      "\n",
      "Test Data: def check(candidate):\n",
      "\n",
      "    # Check some simple cases\n",
      "    assert candidate({\"p\":\"pineapple\", \"b\":\"banana\"}) == True, \"First test error: \" + str(candidate({\"p\":\"pineapple\", \"b\":\"banana\"}))\n",
      "    assert candidate({\"p\":\"pineapple\", \"A\":\"banana\", \"B\":\"banana\"}) == False, \"Second test error: \" + str(candidate({\"p\":\"pineapple\", \"A\":\"banana\", \"B\":\"banana\"}))\n",
      "    assert candidate({\"p\":\"pineapple\", 5:\"banana\", \"a\":\"apple\"}) == False, \"Third test error: \" + str(candidate({\"p\":\"pineapple\", 5:\"banana\", \"a\":\"apple\"}))\n",
      "    assert candidate({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"}) == False, \"Fourth test error: \" + str(candidate({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"}))\n",
      "    assert candidate({\"STATE\":\"NC\", \"ZIP\":\"12345\" }) == True, \"Fifth test error: \" + str(candidate({\"STATE\":\"NC\", \"ZIP\":\"12345\" }))      \n",
      "    assert candidate({\"fruit\":\"Orange\", \"taste\":\"Sweet\" }) == True, \"Fourth test error: \" + str(candidate({\"fruit\":\"Orange\", \"taste\":\"Sweet\" }))      \n",
      "\n",
      "\n",
      "    # Check some edge cases that are easy to work out by hand.\n",
      "    assert candidate({}) == False, \"1st edge test error: \" + str(candidate({}))\n",
      "\n",
      "\n",
      "Entry Point: check_dict_case\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "# Initialize Task Index\n",
    "task_index = 95\n",
    "task_153 = dataset[\"test\"][task_index]\n",
    "\n",
    "# Access Task Details\n",
    "task_id = task_153[\"task_id\"]\n",
    "prompt = task_153[\"prompt\"]\n",
    "canonical_solution = task_153[\"canonical_solution\"]\n",
    "test_data = task_153[\"test\"]\n",
    "entry_point = task_153[\"entry_point\"]\n",
    "\n",
    "# Display Task Details\n",
    "print(f\"Task ID: {task_id}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Canonical Solution: {canonical_solution}\")\n",
    "print(f\"Test Data: {test_data}\")\n",
    "print(f\"Entry Point: {entry_point}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7cb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.00328\n",
      "Execution time Model 1 Output 2: 0.00377\n",
      "Execution time Model 1 Output 3: 0.00308\n",
      "Average Execution Time for Model 1 Output: 0.00338\n",
      "Expected Output:  [True, False, False]\n",
      "Actual Output:  [None, False, False]\n",
      "                 Precision    Recall  F1 Score  Accuracy  Exact Match\n",
      "0                 0.000000  0.000000  0.000000  0.000000     0.000000\n",
      "1                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "2                 1.000000  1.000000  1.000000  1.000000     1.000000\n",
      "Model 1 Average   0.666667  0.666667  0.666667  0.666667     0.666667\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/95\n",
    "# Function - CoT\n",
    "# Model 1\n",
    "\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "def check_dict_case1(dictionary):\n",
    "    # Step 4: Empty Dictionary\n",
    "    if not dictionary:\n",
    "        return False\n",
    "    \n",
    "    # Step 3: Track Consistency\n",
    "    all_lower = None\n",
    "    all_upper = None\n",
    "    \n",
    "    for key in dictionary:\n",
    "        # Step 2: Check Keys\n",
    "        if key.islower():\n",
    "            if all_upper:\n",
    "                return False\n",
    "            all_lower = True\n",
    "        elif key.isupper():\n",
    "            if all_lower:\n",
    "                return False\n",
    "            all_upper = True\n",
    "        else:\n",
    "            # Mixed case keys are not consistent\n",
    "            return False\n",
    "\n",
    "# Test cases\n",
    "mod1_output1 = check_dict_case1({\"p\":\"pineapple\", \"b\":\"banana\"})  # True\n",
    "mod1_output2 = check_dict_case1({\"p\":\"pineapple\", \"A\":\"banana\", \"B\":\"banana\"})  # False\n",
    "mod1_output3 = check_dict_case1({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"})  # False\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod1_output1 = timeit.timeit(lambda: check_dict_case1({\"p\":\"pineapple\", \"b\":\"banana\"}), number=10000)\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "execution_time_mod1_output2 = timeit.timeit(lambda: check_dict_case1({\"p\":\"pineapple\", \"A\":\"banana\", \"B\":\"banana\"}), number=10000)\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "\n",
    "execution_time_mod1_output3 = timeit.timeit(lambda: check_dict_case1({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"}), number=10000)\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [True, False, False]\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20cb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.01309\n",
      "Execution time Model 2 Output 2: 0.00860\n",
      "Execution time Model 2 Output 3: 0.00943\n",
      "Average Execution Time for Model 2 Output: 0.01037\n",
      "Expected Output:  [True, False, False]\n",
      "Actual Output:  [True, False, False]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 2 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/95\n",
    "# Function - CoT\n",
    "# Model 2\n",
    "\n",
    "\n",
    "def check_dict_case2(dictionary):\n",
    "    # Step 1: Define Consistency\n",
    "    lowercase_keys = [key for key in dictionary if key.islower()]\n",
    "    uppercase_keys = [key for key in dictionary if key.isupper()]\n",
    "    \n",
    "    # Step 4: Empty Dictionary\n",
    "    if not lowercase_keys and not uppercase_keys:\n",
    "        return False\n",
    "    elif lowercase_keys and uppercase_keys:\n",
    "        return False\n",
    "    \n",
    "    # Step 5: Return Result\n",
    "    return True\n",
    "\n",
    "\n",
    "# Test cases\n",
    "mod2_output1 = check_dict_case2({\"p\":\"pineapple\", \"b\":\"banana\"})  # True\n",
    "mod2_output2 = check_dict_case2({\"p\":\"pineapple\", \"A\":\"banana\", \"B\":\"banana\"})  # False\n",
    "mod2_output3 = check_dict_case2({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"})  # False\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod2_output1 = timeit.timeit(lambda: check_dict_case2({\"p\":\"pineapple\", \"b\":\"banana\"}), number=10000)\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "execution_time_mod2_output2 = timeit.timeit(lambda: check_dict_case2({\"p\":\"pineapple\", \"A\":\"banana\", \"B\":\"banana\"}), number=10000)\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "execution_time_mod2_output3 = timeit.timeit(lambda: check_dict_case2({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"}), number=10000)\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [True, False, False]\n",
    "actual_outputs_mod2 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod2)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod2)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics2_new = pd.concat([df_metrics2_new, avg_metrics2.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec8f45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.00503\n",
      "Execution time Model 3 Output 2: 0.00778\n",
      "Execution time Model 3 Output 3: 0.00389\n",
      "Average Execution Time for Model 3 Output: 0.00557\n",
      "Expected Output:  [True, False, False]\n",
      "Actual Output:  [True, False, False]\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      1.0     1.0       1.0       1.0          1.0\n",
      "1                      1.0     1.0       1.0       1.0          1.0\n",
      "2                      1.0     1.0       1.0       1.0          1.0\n",
      "Model 3 Average        1.0     1.0       1.0       1.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/95\n",
    "# Function - CoT\n",
    "# Model 3\n",
    "\n",
    "def check_dict_case3(dictionary):\n",
    "    # Step 4: Empty Dictionary\n",
    "    if not dictionary:\n",
    "        return False\n",
    "    \n",
    "    # Step 1: Define Consistency\n",
    "    casing_set = set()\n",
    "    \n",
    "    for key in dictionary:\n",
    "        # Step 2: Check Keys\n",
    "        if key.islower():\n",
    "            casing_set.add('lower')\n",
    "        elif key.isupper():\n",
    "            casing_set.add('upper')\n",
    "        else:\n",
    "            # Mixed case keys are not consistent\n",
    "            return False\n",
    "    \n",
    "    # Step 5: Return Result\n",
    "    return len(casing_set) == 1\n",
    "\n",
    "\n",
    "# Test cases\n",
    "mod3_output1 = check_dict_case3({\"p\":\"pineapple\", \"b\":\"banana\"})  # True\n",
    "mod3_output2 = check_dict_case3({\"p\":\"pineapple\", \"A\":\"banana\", \"B\":\"banana\"})  # False\n",
    "mod3_output3 = check_dict_case3({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"})  # False\n",
    "\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod3_output1 = timeit.timeit(lambda: check_dict_case3({\"p\":\"pineapple\", \"b\":\"banana\"}), number=10000)\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "execution_time_mod3_output2 = timeit.timeit(lambda: check_dict_case3({\"p\":\"pineapple\", \"A\":\"banana\", \"B\":\"banana\"}), number=10000)\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "execution_time_mod3_output3 = timeit.timeit(lambda: check_dict_case3({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"}), number=10000)\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output3, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    if isinstance(expected_output, int) or isinstance(actual_output, int):\n",
    "        # If either expected_output or actual_output is an integer,\n",
    "        # convert them to lists for comparison\n",
    "        expected_output = [expected_output]\n",
    "        actual_output = [actual_output]\n",
    "\n",
    "    if len(expected_output) != len(actual_output):\n",
    "        raise ValueError(\"Expected and actual output lengths must be the same\")\n",
    "\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual)\n",
    "    false_positives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 0 and actual == 1)\n",
    "    false_negatives = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == 1 and actual == 0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for expected, actual in zip(expected_output, actual_output) if expected == actual) / len(expected_output)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = [True, False, False]\n",
    "actual_outputs_mod3 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod3)\n",
    "\n",
    "metrics3 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod3)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics3_new = pd.DataFrame(metrics3)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "avg_metrics3.name = 'Model 3 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics3_new = pd.concat([df_metrics3_new, avg_metrics3.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics3_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec7a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Precision    Recall  F1 Score  Accuracy  Exact Match  \\\n",
      "Model 1                0.666667  0.666667  0.666667  0.666667     0.666667   \n",
      "Model 2                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Model 3                1.000000  1.000000  1.000000  1.000000     1.000000   \n",
      "Overall Average        0.888889  0.888889  0.888889  0.888889     0.888889   \n",
      "Overall Average Time   0.006440  0.006440  0.006440  0.006440     0.006440   \n",
      "\n",
      "                      Average Time  \n",
      "Model 1                   0.003375  \n",
      "Model 2                   0.010374  \n",
      "Model 3                   0.005571  \n",
      "Overall Average           0.006440  \n",
      "Overall Average Time      0.006440  \n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/95\n",
    "# Overall Metrics\n",
    "\n",
    "# Calculate the averages for each set of metrics\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "\n",
    "# Calculate the overall average time\n",
    "overall_average_time = (average_time_mod1output + average_time_mod2output + average_time_mod3output) / 3\n",
    "\n",
    "# Create a DataFrame to store the averages\n",
    "df_avg_metrics = pd.DataFrame([avg_metrics1, avg_metrics2, avg_metrics3])\n",
    "\n",
    "# Add a new column for average time\n",
    "df_avg_metrics['Average Time'] = [average_time_mod1output, average_time_mod2output, average_time_mod3output]\n",
    "\n",
    "# Add a new row displaying overall average\n",
    "df_avg_metrics.loc['Overall Average'] = df_avg_metrics.mean()\n",
    "\n",
    "# Add a new row for overall average time\n",
    "df_avg_metrics.loc['Overall Average Time'] = overall_average_time\n",
    "\n",
    "# Rename the index to include model names\n",
    "df_avg_metrics.index = ['Model 1', 'Model 2', 'Model 3', 'Overall Average', 'Overall Average Time']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ec726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bfc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
