{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796d5735",
   "metadata": {},
   "source": [
    "<h2 style='text-align: center;'> Technology Capstone Research Project PG (11522) </h2>\n",
    "<h3 style='text-align: center;'> Project ID: 2024-S1-50 </h3>\n",
    "<h3 style='text-align: center;'> Group ID: 11522-24S1-41 </h3>\n",
    "<h4 style='text-align: center;'> Member: Pauline Armamento - U3246782 </h4>\n",
    "<h4 style='text-align: center;'> Prompt Technique: Zero-shot </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f361853",
   "metadata": {},
   "source": [
    "### HumanEval\n",
    "\n",
    "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation models.\n",
    "https://huggingface.co/datasets/openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08326a",
   "metadata": {},
   "source": [
    "### Prompt for Zero-Shot\n",
    "\n",
    "# Task ID 153\n",
    "Prompt: Write a Python function that takes the input of class name and list of extensions. Find the extension with the highest strength, calculated by the difference between uppercase and lowercase letters. Join the class name with the strongest extension using a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3930bc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: HumanEval/153\n",
      "Prompt: \n",
      "def Strongest_Extension(class_name, extensions):\n",
      "    \"\"\"You will be given the name of a class (a string) and a list of extensions.\n",
      "    The extensions are to be used to load additional classes to the class. The\n",
      "    strength of the extension is as follows: Let CAP be the number of the uppercase\n",
      "    letters in the extension's name, and let SM be the number of lowercase letters \n",
      "    in the extension's name, the strength is given by the fraction CAP - SM. \n",
      "    You should find the strongest extension and return a string in this \n",
      "    format: ClassName.StrongestExtensionName.\n",
      "    If there are two or more extensions with the same strength, you should\n",
      "    choose the one that comes first in the list.\n",
      "    For example, if you are given \"Slices\" as the class and a list of the\n",
      "    extensions: ['SErviNGSliCes', 'Cheese', 'StuFfed'] then you should\n",
      "    return 'Slices.SErviNGSliCes' since 'SErviNGSliCes' is the strongest extension \n",
      "    (its strength is -1).\n",
      "    Example:\n",
      "    for Strongest_Extension('my_class', ['AA', 'Be', 'CC']) == 'my_class.AA'\n",
      "    \"\"\"\n",
      "\n",
      "Canonical Solution:     strong = extensions[0]\n",
      "    my_val = len([x for x in extensions[0] if x.isalpha() and x.isupper()]) - len([x for x in extensions[0] if x.isalpha() and x.islower()])\n",
      "    for s in extensions:\n",
      "        val = len([x for x in s if x.isalpha() and x.isupper()]) - len([x for x in s if x.isalpha() and x.islower()])\n",
      "        if val > my_val:\n",
      "            strong = s\n",
      "            my_val = val\n",
      "\n",
      "    ans = class_name + \".\" + strong\n",
      "    return ans\n",
      "\n",
      "\n",
      "Test Data: def check(candidate):\n",
      "\n",
      "    # Check some simple cases\n",
      "    assert candidate('Watashi', ['tEN', 'niNE', 'eIGHt8OKe']) == 'Watashi.eIGHt8OKe'\n",
      "    assert candidate('Boku123', ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']) == 'Boku123.YEs.WeCaNe'\n",
      "    assert candidate('__YESIMHERE', ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']) == '__YESIMHERE.NuLl__'\n",
      "    assert candidate('K', ['Ta', 'TAR', 't234An', 'cosSo']) == 'K.TAR'\n",
      "    assert candidate('__HAHA', ['Tab', '123', '781345', '-_-']) == '__HAHA.123'\n",
      "    assert candidate('YameRore', ['HhAas', 'okIWILL123', 'WorkOut', 'Fails', '-_-']) == 'YameRore.okIWILL123'\n",
      "    assert candidate('finNNalLLly', ['Die', 'NowW', 'Wow', 'WoW']) == 'finNNalLLly.WoW'\n",
      "\n",
      "    # Check some edge cases that are easy to work out by hand.\n",
      "    assert candidate('_', ['Bb', '91245']) == '_.Bb'\n",
      "    assert candidate('Sp', ['671235', 'Bb']) == 'Sp.671235'\n",
      "    \n",
      "\n",
      "Entry Point: Strongest_Extension\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "# Initialize Task Index\n",
    "task_index = 153\n",
    "task_153 = dataset[\"test\"][task_index]\n",
    "\n",
    "# Access Task Details\n",
    "task_id = task_153[\"task_id\"]\n",
    "prompt = task_153[\"prompt\"]\n",
    "canonical_solution = task_153[\"canonical_solution\"]\n",
    "test_data = task_153[\"test\"]\n",
    "entry_point = task_153[\"entry_point\"]\n",
    "\n",
    "# Display Task Details\n",
    "print(f\"Task ID: {task_id}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Canonical Solution: {canonical_solution}\")\n",
    "print(f\"Test Data: {test_data}\")\n",
    "print(f\"Entry Point: {entry_point}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efd0f2",
   "metadata": {},
   "source": [
    "#### Revised Prompt for Zero-Shot \n",
    "\n",
    "Write a Python code snippet that does not use a function or function definition that takes the input of class name and list of extensions. Find the extension with the highest strength, calculated by the difference between uppercase and lowercase letters. Join the class name with the strongest extension using a period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c881df",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af90b10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teIGHt8OKe\n",
      "Execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 1 - Test Data 1\n",
    "\n",
    "import time\n",
    "\n",
    "class_names1 = ('Watashi')\n",
    "extensions1 = ['tEN', 'niNE', 'eIGHt8OKe']\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol1_output1 = time.time()\n",
    "\n",
    "strongest_extension1 = max(extensions1, key=lambda ext: sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower()))\n",
    "\n",
    "strongest_index1 = extensions1.index(strongest_extension1)\n",
    "mod1_result1 = class_names1[strongest_index1] + strongest_extension1\n",
    "\n",
    "# Record the ending time\n",
    "end_time_mol1_output1 = time.time()\n",
    "\n",
    "# Print the result\n",
    "print(mod1_result1)\n",
    "mod1_output1 = mod1_result1\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(f\"Execution time: {execution_time_mod1_output1} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2160db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kYEs.WeCaNe\n",
      "Execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 1 - Test Data 2\n",
    "\n",
    "import time\n",
    "\n",
    "class_names2 = ('Boku123')\n",
    "extensions2 = ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol1_output2 = time.time()\n",
    "\n",
    "strongest_extension2 = max(extensions2, key=lambda ext: sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower()))\n",
    "\n",
    "strongest_index2 = extensions2.index(strongest_extension2)\n",
    "mod1_result2 = class_names2[strongest_index2] + strongest_extension2\n",
    "\n",
    "# Record the ending time\n",
    "end_time_mol1_output2 = time.time()\n",
    "\n",
    "# Print the result\n",
    "print(mod1_result2)\n",
    "mod1_output2 = mod1_result2\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(f\"Execution time: {execution_time_mod1_output2} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7dac7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNuLl__\n",
      "Execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 1 - Test Data 3\n",
    "\n",
    "import time\n",
    "\n",
    "class_names3 = ('__YESIMHERE')\n",
    "extensions3 = ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol1_output3 = time.time()\n",
    "\n",
    "strongest_extension3 = max(extensions3, key=lambda ext: sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower()))\n",
    "\n",
    "strongest_index3 = extensions3.index(strongest_extension3)\n",
    "mod1_result3 = class_names3[strongest_index3] + strongest_extension3\n",
    "\n",
    "# Record the ending time\n",
    "end_time_mol1_output3 = time.time()\n",
    "\n",
    "# Print the result\n",
    "print(mod1_result3)\n",
    "mod1_output3 = mod1_result3\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(f\"Execution time: {execution_time_mod1_output3} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c36caef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 1 Output 1: 0.00000\n",
      "Execution time Model 1 Output 2: 0.00000\n",
      "Execution time Model 1 Output 3: 0.00000\n",
      "Average Execution Time for Model 1 Output: 0.00000\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['teIGHt8OKe', 'kYEs.WeCaNe', 'SNuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      0.0     0.0       0.0       0.0          0.0\n",
      "1                      0.0     0.0       0.0       0.0          0.0\n",
      "2                      0.0     0.0       0.0       0.0          0.0\n",
      "Model 1 Average        0.0     0.0       0.0       0.0          0.0\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Function - Zero Shot\n",
    "# Overall Metrics\n",
    "\n",
    "## Model 1 summary\n",
    "\n",
    "# import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# # Execution Time\n",
    "#execution_time_mod1_output1 = end_time_mol1_output1 - start_time_mol1_output1\n",
    "print(\"Execution time Model 1 Output 1:\", format(execution_time_mod1_output1, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output2 = end_time_mol1_output2 - start_time_mol1_output2\n",
    "print(\"Execution time Model 1 Output 2:\", format(execution_time_mod1_output2, '.5f'))\n",
    "\n",
    "#execution_time_mod1_output3 = end_time_mol1_output3 - start_time_mol1_output3\n",
    "print(\"Execution time Model 1 Output 3:\", format(execution_time_mod1_output3, '.5f'))\n",
    "\n",
    "average_time_mod1output = (execution_time_mod1_output1 + execution_time_mod1_output2 + execution_time_mod1_output3) / 3\n",
    "print(\"Average Execution Time for Model 1 Output:\", format(average_time_mod1output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "    false_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 1 and actual_output[i] == 0)\n",
    "    false_negatives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 0 and actual_output[i] == 1)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "expected_outputs = ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
    "actual_outputs_mod1 = [mod1_output1, mod1_output2, mod1_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod1)\n",
    "\n",
    "metrics1 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod1)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics1_new = pd.DataFrame(metrics1)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics1.name = 'Model 1 Average'\n",
    "\n",
    "# # Append the average row to the DataFrame\n",
    "# df_metrics1_new = df_metrics1_new.append(avg_metrics1)\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics1_new = pd.concat([df_metrics1_new, avg_metrics1.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics1_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011c623",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201ca440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teIGHt8OKe\n",
      "Execution time: 0.0009970664978027344 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 2 - Test Data 1\n",
    "\n",
    "import time\n",
    "\n",
    "class_names1 = ('Watashi')\n",
    "extensions1 = ['tEN', 'niNE', 'eIGHt8OKe']\n",
    "\n",
    "max_strength1 = -1\n",
    "strongest_extension_mod2_data1 = \"\"\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol2_output1 = time.time()\n",
    "\n",
    "for ext in extensions1:\n",
    "    strength1 = sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower())\n",
    "    if strength1 > max_strength1:\n",
    "        max_strength1 = strength1\n",
    "        strongest_extension_mod2_data1 = ext\n",
    "\n",
    "strongest_index_mod2_data1 = extensions1.index(strongest_extension_mod2_data1)\n",
    "mod2_result1 = class_names1[strongest_index_mod2_data1] + strongest_extension_mod2_data1\n",
    "\n",
    "# Record the ending time\n",
    "end_time_mol2_output1 = time.time()\n",
    "\n",
    "# Print the result\n",
    "print(mod2_result1)\n",
    "mod2_output1 = mod2_result1\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod2_output1 = end_time_mol2_output1 - start_time_mol2_output1\n",
    "print(f\"Execution time: {execution_time_mod2_output1} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392322a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kYEs.WeCaNe\n",
      "Execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 2 - Test Data 2\n",
    "\n",
    "import time\n",
    "\n",
    "class_names2 = ('Boku123')\n",
    "extensions2 = ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']\n",
    "\n",
    "max_strength2 = -1\n",
    "strongest_extension_mod2_data2 = \"\"\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol2_output2 = time.time()\n",
    "\n",
    "for ext in extensions2:\n",
    "    strength2 = sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower())\n",
    "    if strength2 > max_strength2:\n",
    "        max_strength2 = strength2\n",
    "        strongest_extension_mod2_data2 = ext\n",
    "\n",
    "strongest_index_mod2_data2 = extensions2.index(strongest_extension_mod2_data2)\n",
    "mod2_result2 = class_names2[strongest_index_mod2_data2] + strongest_extension_mod2_data2\n",
    "\n",
    "# Record the ending time\n",
    "end_time_mol2_output2 = time.time()\n",
    "\n",
    "# Print the result\n",
    "print(mod2_result2)\n",
    "mod2_output2 = mod2_result2\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod2_output2 = end_time_mol2_output2 - start_time_mol2_output2\n",
    "print(f\"Execution time: {execution_time_mod2_output2} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6be4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNuLl__\n",
      "Execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 2 - Test Data 3\n",
    "\n",
    "import time\n",
    "\n",
    "class_names3 = ('__YESIMHERE')\n",
    "extensions3 = ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']\n",
    "\n",
    "max_strength3 = -1\n",
    "strongest_extension_mod2_data3 = \"\"\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol2_output3 = time.time()\n",
    "\n",
    "for ext in extensions3:\n",
    "    strength3 = sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower())\n",
    "    if strength3 > max_strength3:\n",
    "        max_strength3 = strength3\n",
    "        strongest_extension_mod2_data3 = ext\n",
    "\n",
    "strongest_index_mod2_data3 = extensions3.index(strongest_extension_mod2_data3)\n",
    "mod2_result3 = class_names3[strongest_index_mod2_data3] + strongest_extension_mod2_data3\n",
    "\n",
    "# Record the ending time\n",
    "end_time_mol2_output3 = time.time()\n",
    "\n",
    "# Print the result\n",
    "print(mod2_result3)\n",
    "mod2_output3 = mod2_result3\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod2_output3 = end_time_mol2_output3 - start_time_mol2_output3\n",
    "print(f\"Execution time: {execution_time_mod2_output3} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee07ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 2 Output 1: 0.00100\n",
      "Execution time Model 2 Output 2: 0.00000\n",
      "Execution time Model 2 Output 3: 0.00000\n",
      "Average Execution Time for Model 2 Output: 0.00033\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['teIGHt8OKe', 'kYEs.WeCaNe', 'SNuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      0.0     0.0       0.0       0.0          0.0\n",
      "1                      0.0     0.0       0.0       0.0          0.0\n",
      "2                      0.0     0.0       0.0       0.0          0.0\n",
      "Model 2 Average        0.0     0.0       0.0       0.0          0.0\n"
     ]
    }
   ],
   "source": [
    "# Model 2 summary\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod2_output1 = end_time_mol2_output1 - start_time_mol2_output1\n",
    "print(\"Execution time Model 2 Output 1:\", format(execution_time_mod2_output1, '.5f'))\n",
    "\n",
    "execution_time_mod2_output2 = end_time_mol2_output2 - start_time_mol2_output2\n",
    "print(\"Execution time Model 2 Output 2:\", format(execution_time_mod2_output2, '.5f'))\n",
    "\n",
    "execution_time_mod2_output3 = end_time_mol2_output3 - start_time_mol2_output3\n",
    "print(\"Execution time Model 2 Output 3:\", format(execution_time_mod2_output3, '.5f'))\n",
    "\n",
    "average_time_mod2output = (execution_time_mod2_output1 + execution_time_mod2_output2 + execution_time_mod2_output3) / 3\n",
    "print(\"Average Execution Time for Model 2 Output:\", format(average_time_mod2output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "    false_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 1 and actual_output[i] == 0)\n",
    "    false_negatives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 0 and actual_output[i] == 1)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "actual_outputs_mod2 = [mod2_output1, mod2_output2, mod2_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod2)\n",
    "\n",
    "metrics2 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod2)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics2_new = pd.DataFrame(metrics2)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics2.name = 'Model 2 Average'\n",
    "\n",
    "# Append the average row to the DataFrame\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics2_new = pd.concat([df_metrics2_new, avg_metrics2.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics2_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15438ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teIGHt8OKe\n",
      "Execution time: 0.0011136531829833984 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 3 - Test Data 1\n",
    "\n",
    "import time\n",
    "\n",
    "class_names1 = ('Watashi')\n",
    "extensions1 = ['tEN', 'niNE', 'eIGHt8OKe']\n",
    "\n",
    "max_strength_mod3_data1 = -1\n",
    "strongest_extension_index_mod3_data1 = -1\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol3_output1 = time.time()\n",
    "\n",
    "for i, ext in enumerate(extensions1):\n",
    "    strength_mod3_data1 = sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower())\n",
    "    if strength_mod3_data1 > max_strength_mod3_data1:\n",
    "        max_strength_mod3_data1 = strength_mod3_data1\n",
    "        strongest_extension_index_mod3_data1 = i\n",
    "\n",
    "if strongest_extension_index_mod3_data1 != -1:\n",
    "    result_mod3_data1 = class_names1[strongest_extension_index_mod3_data1] + extensions1[strongest_extension_index_mod3_data1]\n",
    "    print(result_mod3_data1)\n",
    "else:\n",
    "    print(\"No extensions found.\")\n",
    "    \n",
    "# Record the ending time\n",
    "end_time_mol3_output1 = time.time()\n",
    "\n",
    "# Print the result\n",
    "# print(result_mod3_data1)\n",
    "mod3_output1 = result_mod3_data1\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod3_output1 = end_time_mol3_output1 - start_time_mol3_output1\n",
    "print(f\"Execution time: {execution_time_mod3_output1} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6b5f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kYEs.WeCaNe\n",
      "Execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 3 - Test Data 2\n",
    "\n",
    "import time\n",
    "\n",
    "class_names2 = ('Boku123')\n",
    "extensions2 = ['nani', 'NazeDa', 'YEs.WeCaNe', '32145tggg']\n",
    "\n",
    "max_strength_mod3_data2 = -1\n",
    "strongest_extension_index_mod3_data2 = -1\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol3_output2 = time.time()\n",
    "\n",
    "for i, ext in enumerate(extensions2):\n",
    "    strength_mod3_data2 = sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower())\n",
    "    if strength_mod3_data2 > max_strength_mod3_data2:\n",
    "        max_strength_mod3_data2 = strength_mod3_data2\n",
    "        strongest_extension_index_mod3_data2 = i\n",
    "\n",
    "if strongest_extension_index_mod3_data2 != -1:\n",
    "    result_mod3_data2 = class_names2[strongest_extension_index_mod3_data2] + extensions2[strongest_extension_index_mod3_data2]\n",
    "    print(result_mod3_data2)\n",
    "else:\n",
    "    print(\"No extensions found.\")\n",
    "    \n",
    "# Record the ending time\n",
    "end_time_mol3_output2 = time.time()\n",
    "\n",
    "# Print the result\n",
    "# print(result_mod3_data1)\n",
    "mod3_output2 = result_mod3_data2\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod3_output2 = end_time_mol3_output2 - start_time_mol3_output2\n",
    "print(f\"Execution time: {execution_time_mod3_output2} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94831acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNuLl__\n",
      "Execution time: 0.0010280609130859375 seconds\n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Code Snippet - Zero Shot\n",
    "# Model 3 - Test Data 3\n",
    "\n",
    "import time\n",
    "\n",
    "class_names3 = ('__YESIMHERE')\n",
    "extensions3 = ['t', 'eMptY', 'nothing', 'zeR00', 'NuLl__', '123NoooneB321']\n",
    "\n",
    "max_strength_mod3_data3 = -1\n",
    "strongest_extension_index_mod3_data3 = -1\n",
    "\n",
    "# Record the starting time\n",
    "start_time_mol3_output3 = time.time()\n",
    "\n",
    "for i, ext in enumerate(extensions3):\n",
    "    strength_mod3_data3 = sum(1 for char in ext if char.isupper()) - sum(1 for char in ext if char.islower())\n",
    "    if strength_mod3_data3 > max_strength_mod3_data3:\n",
    "        max_strength_mod3_data3 = strength_mod3_data3\n",
    "        strongest_extension_index_mod3_data3 = i\n",
    "\n",
    "if strongest_extension_index_mod3_data3 != -1:\n",
    "    result_mod3_data3 = class_names3[strongest_extension_index_mod3_data3] + extensions3[strongest_extension_index_mod3_data3]\n",
    "    print(result_mod3_data3)\n",
    "else:\n",
    "    print(\"No extensions found.\")\n",
    "    \n",
    "# Record the ending time\n",
    "end_time_mol3_output3 = time.time()\n",
    "\n",
    "# Print the result\n",
    "# print(result_mod3_data1)\n",
    "mod3_output3 = result_mod3_data3\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time_mod3_output3 = end_time_mol3_output3 - start_time_mol3_output3\n",
    "print(f\"Execution time: {execution_time_mod3_output3} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d085ca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time Model 3 Output 1: 0.00111\n",
      "Execution time Model 3 Output 2: 0.00000\n",
      "Execution time Model 3 Output 3: 0.00103\n",
      "Average Execution Time for Model 3 Output: 0.00071\n",
      "Expected Output:  ['Watashi.eIGHt8OKe', 'Boku123.YEs.WeCaNe', '__YESIMHERE.NuLl__']\n",
      "Actual Output:  ['teIGHt8OKe', 'kYEs.WeCaNe', 'SNuLl__']\n",
      "                 Precision  Recall  F1 Score  Accuracy  Exact Match\n",
      "0                      0.0     0.0       0.0       0.0          0.0\n",
      "1                      0.0     0.0       0.0       0.0          0.0\n",
      "2                      0.0     0.0       0.0       0.0          0.0\n",
      "Model 3 Average        0.0     0.0       0.0       0.0          0.0\n"
     ]
    }
   ],
   "source": [
    "# Model 3 summary\n",
    "\n",
    "# Execution Time\n",
    "execution_time_mod3_output1 = end_time_mol3_output1 - start_time_mol3_output1\n",
    "print(\"Execution time Model 3 Output 1:\", format(execution_time_mod3_output1, '.5f'))\n",
    "\n",
    "execution_time_mod3_output2 = end_time_mol3_output2 - start_time_mol3_output2\n",
    "print(\"Execution time Model 3 Output 2:\", format(execution_time_mod3_output2, '.5f'))\n",
    "\n",
    "execution_time_mod3_output3 = end_time_mol3_output3 - start_time_mol3_output3\n",
    "print(\"Execution time Model 3 Output 3:\", format(execution_time_mod3_output3, '.5f'))\n",
    "\n",
    "average_time_mod3output = (execution_time_mod3_output1 + execution_time_mod3_output2 + execution_time_mod3_output3) / 3\n",
    "print(\"Average Execution Time for Model 3 Output:\", format(average_time_mod3output, '.5f'))\n",
    "\n",
    "def evaluate_list_metrics(expected_output, actual_output):\n",
    "    exact_match_accuracy = 1 if expected_output == actual_output else 0\n",
    "\n",
    "    true_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i])\n",
    "    false_positives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 1 and actual_output[i] == 0)\n",
    "    false_negatives = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == 0 and actual_output[i] == 1)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    accuracy = sum(1 for i in range(min(len(expected_output), len(actual_output))) if expected_output[i] == actual_output[i]) / max(len(expected_output), len(actual_output))\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Exact Match\": exact_match_accuracy\n",
    "    }\n",
    "\n",
    "actual_outputs_mod3 = [mod3_output1, mod3_output2, mod3_output3]\n",
    "print(\"Expected Output: \", expected_outputs)\n",
    "print(\"Actual Output: \", actual_outputs_mod3)\n",
    "\n",
    "metrics3 = [evaluate_list_metrics(expected, actual) for expected, actual in zip(expected_outputs, actual_outputs_mod3)]\n",
    "\n",
    "# Create DataFrame directly from the list of dictionaries\n",
    "df_metrics3_new = pd.DataFrame(metrics3)\n",
    "\n",
    "# Calculate the average for precision, recall, F1 score, accuracy, and exact match\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "avg_metrics3.name = 'Model 3 Average'\n",
    "\n",
    "# Concatenate the average row to the DataFrame\n",
    "df_metrics3_new = pd.concat([df_metrics3_new, avg_metrics3.to_frame().transpose()])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_metrics3_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf18da7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Precision    Recall  F1 Score  Accuracy  Exact Match  \\\n",
      "Model 1                0.000000  0.000000  0.000000  0.000000     0.000000   \n",
      "Model 2                0.000000  0.000000  0.000000  0.000000     0.000000   \n",
      "Model 3                0.000000  0.000000  0.000000  0.000000     0.000000   \n",
      "Overall Average        0.000000  0.000000  0.000000  0.000000     0.000000   \n",
      "Overall Average Time   0.000349  0.000349  0.000349  0.000349     0.000349   \n",
      "\n",
      "                      Average Time  \n",
      "Model 1                   0.000000  \n",
      "Model 2                   0.000332  \n",
      "Model 3                   0.000714  \n",
      "Overall Average           0.000349  \n",
      "Overall Average Time      0.000349  \n"
     ]
    }
   ],
   "source": [
    "# Task ID: HumanEval/153\n",
    "# Overall Metrics\n",
    "\n",
    "# Calculate the averages for each set of metrics\n",
    "avg_metrics1 = df_metrics1_new.mean(axis=0)\n",
    "avg_metrics2 = df_metrics2_new.mean(axis=0)\n",
    "avg_metrics3 = df_metrics3_new.mean(axis=0)\n",
    "\n",
    "# Calculate the overall average time\n",
    "overall_average_time = (average_time_mod1output + average_time_mod2output + average_time_mod3output) / 3\n",
    "\n",
    "# Create a DataFrame to store the averages\n",
    "df_avg_metrics = pd.DataFrame([avg_metrics1, avg_metrics2, avg_metrics3])\n",
    "\n",
    "# Add a new column for average time\n",
    "df_avg_metrics['Average Time'] = [average_time_mod1output, average_time_mod2output, average_time_mod3output]\n",
    "\n",
    "# Add a new row displaying overall average\n",
    "df_avg_metrics.loc['Overall Average'] = df_avg_metrics.mean()\n",
    "\n",
    "# Add a new row for overall average time\n",
    "df_avg_metrics.loc['Overall Average Time'] = overall_average_time\n",
    "\n",
    "# Rename the index to include model names\n",
    "df_avg_metrics.index = ['Model 1', 'Model 2', 'Model 3', 'Overall Average', 'Overall Average Time']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1a294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
